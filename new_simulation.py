#!/usr/bin/env python3
"""
simulation.py  –  Synchronous Producer
======================================

•   Re‑plays historical candles and writes four "live" CSV files
    (M5 / M15 / M30 / H1) for each iteration.
•   Optionally creates an *X_live* snapshot (`X_live_snapshot.csv`)
    right after the CSVs are written (use --dump-snapshot).
•   Waits for `Answer.txt` (generated by your live predictor), updates
    accuracy stats, then deletes the live CSV files.

Run
----
    python simulation.py                   # classic behaviour
    python simulation.py --dump-snapshot   # write snapshot each loop
"""

# ─────────────────────────  Imports
import argparse
import logging
import multiprocessing as mp
import os
import sys
import time
import json
import pandas as pd
import joblib

from prepare_data_for_train import PREPARE_DATA_FOR_TRAIN

# ─────────────────────────  Constants
MAX_ROWS       = 4_999
SLEEP_TIME     = 1          # s
TIMEOUT_ANSWER = 300        # s

LIVE_M30 = "XAUUSD.F_M30_live.csv"
LIVE_M15 = "XAUUSD.F_M15_live.csv"
LIVE_M5  = "XAUUSD.F_M5_live.csv"
LIVE_H1  = "XAUUSD.F_H1_live.csv"
ANSWER_TXT = "Answer.txt"

REPORT_CSV = "simulation_report.csv"
LOG_FILE   = "simulation.log"

# ╔═══════════════════════════════════════════════════════════════════════════╗
#  Utility helpers
# ╚═══════════════════════════════════════════════════════════════════════════╝
def rm_if_exists(path: str) -> None:
    if os.path.exists(path):
        os.remove(path)

def purge_leftovers() -> None:
    """Remove live CSVs & Answer.txt from previous runs."""
    for fp in (LIVE_M30, LIVE_M15, LIVE_M5, LIVE_H1, ANSWER_TXT):
        rm_if_exists(fp)

def setup_logger(level=logging.INFO) -> None:
    fmt_file = "%(asctime)s [%(levelname)s] %(message)s"
    fmt_cli  = "%(asctime)s [%(levelname)s] %(message)s"

    fh = logging.FileHandler(LOG_FILE, mode="w")
    fh.setLevel(level)
    fh.setFormatter(logging.Formatter(fmt_file, "%Y-%m-%d %H:%M:%S"))

    sh = logging.StreamHandler(sys.stdout)
    sh.setLevel(level)
    sh.setFormatter(logging.Formatter(fmt_cli, "%H:%M:%S"))

    logging.basicConfig(level=level, handlers=[fh, sh])
    logging.info(">>> Simulation started")

def load_and_sort(csv_path: str) -> pd.DataFrame:
    df = pd.read_csv(csv_path)
    df["time"] = pd.to_datetime(df["time"])
    df.sort_values("time", inplace=True, ignore_index=True)
    logging.debug(f"Loaded {csv_path}  shape={df.shape}")
    return df

def index_of_time(df: pd.DataFrame, t, tf: str) -> int:
    """Binary‑search like index lookup aligned to timeframe rules."""
    if tf == "H1":
        t = t.replace(minute=0, second=0, microsecond=0)
    elif tf == "M15":
        t = t.replace(minute=(t.minute // 15)*15, second=0, microsecond=0)
    elif tf == "M5":
        t = t.replace(minute=(t.minute // 5)*5, second=0, microsecond=0)

    idx = df["time"].searchsorted(t, side="left")
    if idx == len(df) or df.iloc[idx]["time"] != t:
        return -1
    return idx

def slice_to_file(df: pd.DataFrame, idx: int, out_path: str) -> None:
    df.iloc[:idx+1].tail(MAX_ROWS).to_csv(out_path, index=False)

def write_tf_csvs(ts, df5, df15, df30, dfh1) -> bool:
    tasks = [
        (df5,  ts, "M5",  LIVE_M5),
        (df15, ts, "M15", LIVE_M15),
        (df30, ts, "M30", LIVE_M30),
        (dfh1, ts, "H1",  LIVE_H1),
    ]
    ok = True
    with mp.Pool(processes=4) as pool:
        res = [pool.apply_async(_one_tf_job, t) for t in tasks]
        pool.close(); pool.join()
        for r in res:
            ok &= r.get()
    return ok

def _one_tf_job(df, ts, tf, out_path):
    idx = index_of_time(df, ts, tf)
    if idx == -1:
        logging.warning(f"{tf}: timestamp {ts} not found")
        return False
    slice_to_file(df, idx, out_path)
    return True

def wait_until_live_deleted():
    while any(os.path.exists(fp)
              for fp in (LIVE_M30, LIVE_M15, LIVE_M5, LIVE_H1)):
        time.sleep(SLEEP_TIME)

def wait_for_answer() -> tuple[str, float]:
    waited = 0
    while not os.path.exists(ANSWER_TXT):
        time.sleep(SLEEP_TIME)
        waited += SLEEP_TIME
        if waited >= TIMEOUT_ANSWER:
            logging.warning("Timed‑out waiting for Answer.txt")
            return "NAN", 0.0
    txt = open(ANSWER_TXT).read().strip().split(",")
    rm_if_exists(ANSWER_TXT)
    if len(txt) == 2:
        return txt[0].strip().upper(), float(txt[1])
    return txt[0].strip().upper(), 0.0

# ╔═══════════════════════════════════════════════════════════════════════════╗
#  Snapshot helper  (simplified copy of save_live_features logic)
# ╚═══════════════════════════════════════════════════════════════════════════╝
def write_snapshot(model_path: str, out_csv: str) -> bool:
    """Return True on success, False otherwise (never raises)."""
    try:
        saved = joblib.load(model_path)
        window   = saved["window_size"]
        feats    = saved["feats"]
        columns  = saved["train_window_cols"]
        raw_win  = saved.get("train_raw_window")
    except Exception as e:
        logging.error(f"[snapshot] Cannot load model: {e}")
        return False

    # build PREPARE_DATA_FOR_TRAIN on‑the‑fly
    file_dict = {
        "30T": LIVE_M30,
        "1H" : LIVE_H1,
        "15T": LIVE_M15,
        "5T" : LIVE_M5,
    }
    prep = PREPARE_DATA_FOR_TRAIN(filepaths=file_dict, main_timeframe="30T")
    try:
        merged = prep.load_data()
    except Exception as e:
        logging.error(f"[snapshot] load_data failed: {e}")
        return False

    needed = window + 1
    if raw_win is not None:
        merged = pd.concat([raw_win, merged], ignore_index=True).tail(needed)
    else:
        merged = merged.tail(needed)

    try:
        if window == 1:
            X, *_ = prep.ready(merged, window=1, selected_features=feats, mode="predict")
        else:
            X, _  = prep.ready_incremental(merged, window=window, selected_features=feats)
    except Exception as e:
        logging.error(f"[snapshot] Feature build failed: {e}")
        return False

    if X.empty:
        logging.warning("[snapshot] X_live empty – nothing stored.")
        return False

    X = X.fillna(0)
    X.columns = [str(c) for c in X.columns]
    X = X.reindex(columns=columns, fill_value=0).astype(float)

    try:
        X.tail(1).to_csv(out_csv, index=False)
        logging.info(f"[snapshot] Snapshot written ➜ {out_csv}")
        return True
    except Exception as e:
        logging.error(f"[snapshot] Could not save CSV: {e}")
        return False

# ╔═══════════════════════════════════════════════════════════════════════════╗
#  Main simulation loop
# ╚═══════════════════════════════════════════════════════════════════════════╝
def main():
    # ───── CLI
    ap = argparse.ArgumentParser(description="Historical‑to‑live simulator")
    ap.add_argument("--dump-snapshot", action="store_true",
                    help="write X_live_snapshot.csv each iteration")
    ap.add_argument("--model-path", default="best_model.pkl",
                    help="required if --dump-snapshot is set")
    args = ap.parse_args()

    setup_logger()
    purge_leftovers()

    # ───── Load historical data once
    df_m5  = load_and_sort("XAUUSD_M5.csv")
    df_m15 = load_and_sort("XAUUSD_M15.csv")
    df_m30 = load_and_sort("XAUUSD_M30.csv")
    df_h1  = load_and_sort("XAUUSD_H1.csv")

    wins = loses = nan_cnt = 0
    profit = 0.0
    history = []

    total = len(df_m30)
    logging.info(f"M30 rows = {total}")

    for it in range(total - 1):
        idx = total - 1 - it
        if idx < 1:
            break

        wait_until_live_deleted()   # ensure clean slate

        ts_row  = df_m30.iloc[idx]
        ts_next = df_m30.iloc[idx - 1]

        ts   = ts_row["time"]
        c0   = ts_row["close"]
        c1   = ts_next["close"]
        true = "SEL" if c0 - c1 >= 0 else "BUY"
        delta = c0 - c1

        logging.info(f"\n[Sim {it+1}] {ts}  true={true}  Δ={delta:.2f}")

        if not write_tf_csvs(ts, df_m5, df_m15, df_m30, df_h1):
            logging.warning("Some timeframes missing – iteration marked NAN.")
            nan_cnt += 1
            purge_leftovers()
            continue

        # optional snapshot (non‑blocking)
        if args.dump_snapshot:
            write_snapshot(args.model_path, "X_live_snapshot.csv")

        # wait for Answer.txt
        decision, prob = wait_for_answer()

        correct = (decision == true)
        if correct:
            wins += 1
            profit += abs(delta)
        elif decision in ("BUY", "SEL"):
            loses += 1
            profit -= abs(delta)
        else:
            nan_cnt += 1

        acc = wins / (wins + loses) if (wins + loses) else 0.0
        logging.info(
            f"[Sim {it+1}] decision={decision}  prob={prob:.4f}  "
            f"accur={acc:.4f}  W={wins}  L={loses}  NAN={nan_cnt}"
        )

        # record for final report
        history.append({
            "time":         str(ts),
            "decision":     decision,
            "prob":         prob,
            "true":         true,
            "pip_change":   float(delta),
            "correct":      correct,
            "wins":         wins,
            "loses":        loses,
            "nan":          nan_cnt,
            "profit_pips":  profit,
            "cum_acc":      acc
        })

    # ───── save report
    pd.DataFrame(history).to_csv(REPORT_CSV, index=False)
    logging.info(f"Simulation finished – report ➜ {REPORT_CSV}")

# ─────────────────────────────────────────────────────────────────────────────
if __name__ == "__main__":
    main()
