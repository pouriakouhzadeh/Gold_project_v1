#!/usr/bin/env python3
"""
Full dataâ€‘preparation pipeline for GA trainer (legacyâ€‘compatible).
----------------------------------------------------------------
* Ø¨Ø§Ø²ØªÙˆÙ„ÛŒØ¯ Ú©Ø§Ù…Ù„ ØªÙ…Ø§Ù… Ø®Ø·ÙˆØ· Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ Ù†Ø³Ø®Ù‡Ù” Ø§ØµÙ„ÛŒ (â‰ˆâ€¯630â€¯Ø³Ø·Ø±)
* Ø±ÛŒØ³Ø§Ù…Ù¾Ù„ Ø§ÛŒÙ…Ù† Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ù†Ø¯Ù„ Ù†Ø§Ù‚Øµ (first / max / min / last / sum)
* Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø§â€¯TimeSeriesSplitâ€¯+â€¯Mutualâ€¯Informationâ€¯+â€¯Ø­Ø°Ù Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ
* Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø²ÛŒ + Ù¾Ù†Ø¬Ø±Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ù†Ø¹Ø·Ø§Ùâ€ŒÙ¾Ø°ÛŒØ±
* Ø­Ø°ÙÙ ØªØ¹Ø·ÛŒÙ„Ø§Øª (Ø´Ù†Ø¨Ù‡/ÛŒÚ©Ø´Ù†Ø¨Ù‡) Ùˆ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
* Ø¨Ø°Ø± ØªØµØ§Ø¯ÙÛŒ Ø«Ø§Ø¨Øªâ€¯(2025) Ùˆ Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ Ø®Ù„Ø§ØµÙ‡
"""
from __future__ import annotations

import gc
import re
from collections import defaultdict
import logging
import multiprocessing as mp
import warnings
from typing import List, Tuple
from AllIndicatorsNoLeak import AllIndicatorsNoLeak
from custotechIndicators import CustomTechIndicators
import numpy as np
import pandas as pd
from collections import defaultdict, Counter
from leakfree_indicators import LeakFreeBatchLive
from joblib import Parallel, delayed
from numba import config as numba_config
from sklearn.feature_selection import VarianceThreshold, mutual_info_classif
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import MinMaxScaler
from sklearnex import patch_sklearn
from clear_data import ClearData
from DriftBasedStartDateSuggester import DriftBasedStartDateSuggester
from custom_indicators import (
    CustomCandlestickPattern,
    CustomIchimokuIndicator,
    CustomPivotPointIndicator,
    CustomVolumeRateOfChangeIndicator,
    CustomWilliamsRIndicator,
    KSTCustomIndicator,
    VortexCustomIndicator,
)
from numba_utils import (
    numba_kurtosis,
    numba_last_local_max_idx,
    numba_last_local_min_idx,
    numba_median,
    numba_skew,
    numba_up_count,
)
from time_utils import TimeColumnFixer as TFix

patch_sklearn(verbose=False)

# ---------------- LOGGING & WARNINGS ----------------
logging.getLogger("sklearnex").setLevel(logging.WARNING)
logging.getLogger("sklearn").setLevel(logging.WARNING)
logging.getLogger("daal4py").setLevel(logging.WARNING)
logging.getLogger("numba").setLevel(logging.CRITICAL)
numba_config.LOG_LEVEL = "CRITICAL"

warnings.filterwarnings("ignore")
logging.basicConfig(level=logging.INFO, filename="genetic_algorithm.log", format="%(asctime)s %(levelname)s:%(message)s")

# ---------------- REPRODUCIBILITY ----------------
SEED = 2025
np.random.seed(SEED)

# ---------------- HELPERS ----------------

def _timedelta_to_seconds(df: pd.DataFrame):
    for col in df.columns:
        if pd.api.types.is_timedelta64_dtype(df[col]):
            df[col] = df[col].dt.total_seconds()

# ---------------- SAFE RESAMPLE ----------------

def _safe_agg_group(grp: pd.DataFrame, agg_dict: dict[str, callable]):
    if len(grp) >= 2:
        return grp.iloc[:-1].agg(agg_dict).to_frame().T
    if len(grp) == 1:
        return grp.iloc[[0]]
    return None

# ---------------- MAIN CLASS ----------------
class PREPARE_DATA_FOR_TRAIN:
        # ------------------------------------------------------------------
    # LIVE incremental internal state  (keep last two raw rows)
    # ------------------------------------------------------------------
    _live_prev2: pd.DataFrame | None = None       # Ø¢Ø®Ø±ÛŒÙ† Ø¯Ùˆ Ø±Ø¯ÛŒÙ Ø®Ø§Ù…
    _live_prev_time: pd.Timestamp | None = None   # ÙÙ‚Ø· Ø§Ú¯Ø± Ø®ÙˆØ§Ø³ØªÛŒ ØªØ±ØªÛŒØ¨ Ø±Ø§ Ú†Ú© Ú©Ù†ÛŒ

        # --------- NEW ---------
    bad_cols_tf: dict[str, set[str]] = defaultdict(set)   # {"30T": {"colA", ...}, "1H": {...}}
    allow_regex = re.compile(r"(?:is_weekend|day_of_week|hour)$", re.I)
        # ---------- NEW: helpers to unify batch & live -----------------
    def _compute_diff(self, data: pd.DataFrame,
                      feat_cols: list[str],
                      strict_cols: bool) -> pd.DataFrame:
        """
        ÙˆØ§Ø­Ø¯ Ù…Ø±Ú©Ø²ÛŒ diff/shift + Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒØ› Ù‡Ù…Ù‡ Ø¬Ø§ ÙÙ‚Ø· Ø§ÛŒÙ† Ø±Ø§ ØµØ¯Ø§ Ù…ÛŒâ€ŒØ²Ù†ÛŒÙ….
        """
        df = data[feat_cols].shift(1).diff()
        _timedelta_to_seconds(df)           # ØªØ¨Ø¯ÛŒÙ„ timedelta Ø¨Ù‡ Ø«Ø§Ù†ÛŒÙ‡

        if not strict_cols and self.bad_cols_tf:
            bad_union = set().union(*self.bad_cols_tf.values())
            df.drop(columns=[c for c in bad_union if c in df.columns],
                    inplace=True, errors="ignore")

        # Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ ØµÙØ± (ØºÛŒØ±Ø¨Ø§ÛŒÙ†Ø±ÛŒ)
        is_bin = (df.nunique() <= 2).to_dict()
        zero_like = [c for c in df.columns
                    if df[c].abs().max() < 1e-12 and not is_bin.get(c, False)]
        if not strict_cols:                       # â† ÙÙ‚Ø· ÙˆÙ‚ØªÛŒ strict Ù†ÛŒØ³Øª Ø­Ø°Ù Ú©Ù†
            df.drop(columns=zero_like, inplace=True, errors="ignore")


        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        df.ffill(inplace=True)
        df.dropna(how="all", inplace=True)
        df.reset_index(drop=True, inplace=True)
        return df

    def _apply_window(self, X_f: pd.DataFrame, y: pd.Series,
                      feats: list[str], window: int,
                      selected_features: list[str]|None,
                      has_tminus: bool):
        """
        ØªÙˆÙ„ÛŒØ¯ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ _tminus ÙÙ‚Ø· ÛŒÚ©â€ŒØ¬Ø§ØŒ Ø¨Ø±Ø§ÛŒ batch Ùˆ live.
        """
        if window <= 1:
            return X_f, y, feats

        if len(X_f) < window:
            logging.warning("Not enough rows for window=%d", window)
            return pd.DataFrame(), pd.Series(dtype=int), feats

        stacked = np.concatenate(
            [X_f.shift(i).iloc[window-1:].values for i in range(window)],
            axis=1
        )
        X_f = pd.DataFrame(
            stacked,
            columns=[f"{c}_tminus{i}" for i in range(window) for c in feats]
        )
        y = y.iloc[window-1:].reset_index(drop=True)

        if has_tminus and selected_features:
            X_f = X_f[[c for c in selected_features if c in X_f.columns]]

        return X_f, y, feats
    # ---------- END NEW ---------------------------------------------------

    def _detect_bad_cols_tf(
        self,
        df: pd.DataFrame,
        tf: str,
        *,
        windows: tuple[int, ...] = (1,2,3,4,5,6,9,12,20,24,30,34),
        stride: int = 75,
        ratio_thr: float = 0.12,     # â† Ø­ØªÛŒ 12 Ùª Ø®Ø·Ø§ Ú©Ø§ÙÛŒ Ø§Ø³Øª
        min_fail:  int   = 8,        # â† ÛŒØ§ Ø­Ø¯Ø§Ù‚Ù„ 8 Ø¨Ø§Ø± Ø®Ø±Ø§Ø¨ÛŒ
    ) -> None:
        """
        â€¢ bad_zero_nan  : ØµÙØ±/NaN Ø¯Ø± Ø¢Ø®Ø± Ù¾Ù†Ø¬Ø±Ù‡
        â€¢ bad_forward   : ØªØºÛŒÛŒØ± Ø±Ø¯ÛŒÙ Ù…Ø§Ù‚Ø¨Ù„â€ŒØ¢Ø®Ø± Ù¾Ø³ Ø§Ø² ÙˆØ±ÙˆØ¯ Ø±Ú©ÙˆØ±Ø¯ Ø¬Ø¯ÛŒØ¯
        Ø³ØªÙˆÙ† Ø­Ø°Ù Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø§Ú¯Ø±  (fails / tests  >= ratio_thr)  Â«ÛŒØ§Â»  fails >= min_fail
        """

        # â”€â”€â”€ Ø´Ù…Ø§Ø±Ù†Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³ØªÙˆÙ†ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        zero_nan_fail   : Counter[str] = Counter()
        forward_fail    : Counter[str] = Counter()
        zero_nan_tests  : Counter[str] = Counter()
        forward_tests   : Counter[str] = Counter()

        n_segments = 8
        seg_len    = len(df) // n_segments or len(df)

        for seg_idx in range(n_segments):
            seg = df.iloc[seg_idx*seg_len : (seg_idx+1)*seg_len]

            for win in windows:
                if len(seg) <= win:  # Ù¾Ù†Ø¬Ø±Ù‡â€Œ Ø¬Ø§ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯
                    continue

                for start in range(0, len(seg) - win, stride):
                    # â”€â”€ (1) 0 / NaN Ø¯Ø± Ø¢Ø®Ø± Ù¾Ù†Ø¬Ø±Ù‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    last = seg.iloc[start+win-1]
                    bad  = last.isna() | np.isclose(last, 0.0, atol=1e-12)

                    for col in last.index:            # ØªØ³Øª Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø³ØªÙˆÙ†
                        zero_nan_tests[col] += 1
                        if bad[col]:
                            zero_nan_fail[col] += 1

                    # â”€â”€ (2) forward-looking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    if start + win >= len(seg):
                        continue                      # Ø±Ú©ÙˆØ±Ø¯ Ø§Ø¶Ø§ÙÙ‡ Ù†Ø¯Ø§Ø±ÛŒÙ…

                    sub1 = seg.iloc[start:start+win]
                    sub2 = seg.iloc[start:start+win+1]

                    pen_old = sub1.iloc[-1]
                    pen_new = sub2.iloc[-2]

                    changed = (
                        pen_old.notna()
                        & pen_new.notna()
                        & (~np.isclose(pen_old, pen_new,
                                    rtol=1e-6, atol=1e-12))
                    )

                    for col in pen_old.index:
                        forward_tests[col] += 1
                        if changed[col]:
                            forward_fail[col] += 1

        # â”€â”€â”€ ØªØµÙ…ÛŒÙ… Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø³ØªÙˆÙ† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        bad_cols: set[str] = set()

        for col in zero_nan_tests:
            # Ù†Ø³Ø¨ØªÙâ€Œ Ø®Ø·Ø§
            r = zero_nan_fail[col] / zero_nan_tests[col]
            if (r >= ratio_thr) or (zero_nan_fail[col] >= min_fail):
                bad_cols.add(col)

        for col in forward_tests:
            r = forward_fail[col] / forward_tests[col]
            if (r >= ratio_thr) or (forward_fail[col] >= min_fail):
                bad_cols.add(col)

        # â”€â”€â”€ Ø­Ø°Ù Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ø§Ø² (binary / ØªÙ‚ÙˆÛŒÙ…ÛŒ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        bad_cols = {c for c in bad_cols if not self.allow_regex.search(c)}

        # â”€â”€â”€ Ø¨Ù‡ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ú©Ù„Ø§Ø³ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self.bad_cols_tf[tf].update(bad_cols)

        if self.verbose:
            z_bad = len({c for c in bad_cols if c in zero_nan_fail})
            f_bad = len({c for c in bad_cols if c in forward_fail})
            print(f"[DETECT-{tf}] â†‘{len(bad_cols)} cols  "
                f"(0/NaN â‰¥{ratio_thr:.0%} or â‰¥{min_fail} â†’{z_bad}, "
                f"fwd â‰¥{ratio_thr:.0%} or â‰¥{min_fail} â†’{f_bad})")

    def __init__(self, filepaths: dict[str, str] | None = None, main_timeframe="30T", verbose=True, fast_mode: bool = False):
        defaults = {"30T": "XAUUSD_M30.csv", "1H": "XAUUSD_H1.csv", "15T": "XAUUSD_M15.csv", "5T": "XAUUSD_M5.csv"}
        self.filepaths = filepaths or defaults
        self.main_timeframe = main_timeframe
        self.verbose = verbose
        self.fast_mode = fast_mode                         # â† NEW
        self.train_columns_after_window: List[str] = []

        # ÙÙ‚Ø· Ø¯Ø± Ø­Ø§Ù„Øª Ù…Ø¹Ù…ÙˆÙ„ (Train) drift-scan Ø´ÙˆØ¯Ø› Ø¯Ø± fast_mode Ø®Ø§Ù…ÙˆØ´
        self.shared_start_date = None
        if not fast_mode:
            self.drift_finder = DriftBasedStartDateSuggester(self.filepaths)
            self.shared_start_date = self.drift_finder.find_shared_start_date()
            if verbose:
                print(f"ğŸ“… Shared drift-aware training start date: {self.shared_start_date}")

        if verbose:
            print("[PREP] Initialised for", main_timeframe)
        logging.info("[INIT] main_timeframe=%s", self.main_timeframe)

        # ÙÙ‚Ø· ÙˆÙ‚ØªÛŒ drift-scan Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ØŒ Ø¢Ù† Ø±Ø§ Ú†Ø§Ù¾ Ú©Ù†
        if (not self.fast_mode) and (self.shared_start_date is not None):
            print(f"ğŸ“… Shared drift-aware training start date: {self.shared_start_date}")


    # ================= 1) LOAD & FEATURE ENGINEER =================
    def load_and_process_timeframe(self, tf: str, filepath: str) -> pd.DataFrame:
        # print("Load and process time frame start ...")
        df = ClearData().clean(pd.read_csv(filepath))
        if "time" not in df.columns:
            raise ValueError("'time' column missing in CSV")
        df["time"] = pd.to_datetime(df["time"], errors="coerce")
        df.dropna(subset=["time"], inplace=True)
        df.sort_values("time", inplace=True)

        if self.shared_start_date:
            df = df[df["time"] >= self.shared_start_date]
            print(f"[{tf}] â³ Trimmed data from {self.shared_start_date.date()}")
        df.set_index("time", inplace=True)
        if self.verbose:
            print(f"len df {tf} = {len(df)}")
        prefix = f"{tf}_"

        # ---------------- SIMPLE ROLLING ----------------
        # df[f"{prefix}ma20"] = df["close"].rolling(20, 1).mean()
        # df[f"{prefix}ma50"] = df["close"].rolling(50, 1).mean()
        df[f"{prefix}ma_volume20"] = df["volume"].rolling(20, 1).mean()
        # df[f"{prefix}return_difference"] = df["close"].diff()
        # df[f"{prefix}roc"] = df["close"].pct_change() * 100
        df[f"{prefix}rolling_mean_20"] = df["close"].rolling(20, 1).mean()
        df[f"{prefix}rolling_std_20"] = df["close"].rolling(20, 1).std()
        df[f"{prefix}rolling_skew_20"] = df["close"].rolling(20, 1).apply(numba_skew, raw=True)
        df[f"{prefix}rolling_kurt_20"] = df["close"].rolling(20, 1).apply(numba_kurtosis, raw=True)
        df[f"{prefix}rolling_median_20"] = df["close"].rolling(20, 1).apply(numba_median, raw=True)
        df[f"{prefix}rolling_up_count_20"] = df["close"].rolling(20, 1).apply(numba_up_count, raw=True)

        # ---------------- TA FEATURES ----------------
        ta_builder = AllIndicatorsNoLeak(
            df.copy(),           # you can still pass a copy if you like
            o="open", h="high", l="low", c="close", v="volume",
            prefix=prefix        # same prefix you used before
        )
        df = ta_builder.add_features(inplace=True) 
        df = df.loc[:, ~df.columns.duplicated()]
        
        # ---------- LEAKâ€‘FREE replacements for KCP/DCP/BBP/CCI/FI/OBV/ADI/Stochâ€‘RSI/Pivot â€¦ ----------
        safe_ind = LeakFreeBatchLive(df, prefix=prefix,
                                    o="open", h="high", l="low", c="close", v="volume")
        df = pd.concat([df, safe_ind.build()], axis=1)
        df = df.loc[:, ~df.columns.duplicated()]  
        
        # ---------------- MANUAL & CUSTOM INDICATORS ----------------
        # df: Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø®Ø§Ù…ØŒ prefix Ù‡Ù…Ø§Ù† Ù…ØªØºÛŒØ± Ù‚Ø¨Ù„ÛŒ
        ind = CustomTechIndicators(df, prefix=prefix,
                                o="open", h="high", l="low", c="close", v="volume")
        df = ind.add_features(inplace=True)   # Ù‡Ù…Ù‡Ù” Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ø§ ÛŒÚ© ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ
        df = df.loc[:, ~df.columns.duplicated()]

        # ---- CUSTOM INDICATORS ----
        kst = KSTCustomIndicator(df["close"], 10, 15, 20, 30, 10, 10, 10, 15, 9, fillna=True)
        df[f"{prefix}kst_main"] = kst.kst(); df[f"{prefix}kst_signal"] = kst.kst_signal(); df[f"{prefix}kst_diff"] = kst.kst_diff()
        vtx = VortexCustomIndicator(df["high"], df["low"], df["close"], 14, fillna=True)
        df[f"{prefix}vortex_pos"] = vtx.vortex_pos(); df[f"{prefix}vortex_neg"] = vtx.vortex_neg()
        ichi = CustomIchimokuIndicator(df["high"], df["low"], df["close"], 9, 26, 52)
        df[f"{prefix}ichimoku_conversion_line"] = ichi.ichimoku_conversion_line()
        df[f"{prefix}ichimoku_base_line"] = ichi.ichimoku_base_line()
        df[f"{prefix}ichimoku_a"] = (df[f"{prefix}ichimoku_conversion_line"] + df[f"{prefix}ichimoku_base_line"])/2
        df[f"{prefix}ichimoku_b"] = (df["high"].rolling(52).max() + df["low"].rolling(52).min())/2
        df[f"{prefix}williams_r"] = CustomWilliamsRIndicator(df["high"], df["low"], df["close"], 14).williams_r()
        df[f"{prefix}vroc"] = CustomVolumeRateOfChangeIndicator(df["volume"], 20).volume_rate_of_change()
        piv = CustomPivotPointIndicator(df["high"], df["low"], df["close"], 5)
        # df[f"{prefix}pivot"] = piv.pivot(); df[f"{prefix}support_1"] = piv.support_1(); df[f"{prefix}support_2"] = piv.support_2(); df[f"{prefix}resistance_1"] = piv.resistance_1(); df[f"{prefix}resistance_2"] = piv.resistance_2()
        candle = CustomCandlestickPattern(df["open"], df["high"], df["low"], df["close"])
        df[f"{prefix}engulfing"] = candle.engulfing(); df[f"{prefix}doji"] = candle.doji()
        ha_close = (df["open"] + df["high"] + df["low"] + df["close"]) / 4; ha_open = ha_close.shift(1).ffill()
        df[f"{prefix}heikin_ashi_open"] = ha_open; df[f"{prefix}heikin_ashi_close"] = ha_close
        df[f"{prefix}range_close_ratio"] = (df["high"] - df["low"]) / (df["close"] + 1e-9)
        df[f"{prefix}bull_power"] = (df["close"] - df["low"]) / ((df["high"] - df["low"]) + 1e-9)
        df[f"{prefix}bars_from_local_max_20"] = df["close"].rolling(20, 1).apply(numba_last_local_max_idx, raw=True)
        df[f"{prefix}bars_from_local_min_20"] = df["close"].rolling(20, 1).apply(numba_last_local_min_idx, raw=True)
        df[f"{prefix}rsi_macd"] = df[f"{prefix}rsi_14"] * df[f"{prefix}macd"]
        # df[f"{prefix}ma20_ma50_ratio"] = df[f"{prefix}ma20"] / (df[f"{prefix}ma50"] + 1e-9)
        # ---------- Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…Ù Ù¾ÛŒØ´ÙˆÙ†Ø¯Ø¯Ø§Ø± ----------
        for base_col in ("open", "high", "low", "close", "volume"):
            pref_col = f"{prefix}{base_col}"
            if pref_col not in df.columns:
                df[pref_col] = df[base_col]

        df.replace([np.inf, -np.inf], np.nan, inplace=True); df.ffill(inplace=True); df.dropna(how="all", inplace=True)
        
        # ---- NEW: detect bad cols for this timeframe BEFORE resample ----
        if not getattr(self, "fast_mode", False):
            self._detect_bad_cols_tf(df, tf)

        # ---------------- SAFE RESAMPLE ----------------
        # print("Safe resample start ...")
        if tf != self.main_timeframe:
            base_aggs = {
                "open": lambda x: x.iloc[0] if not x.empty else np.nan,
                "high": lambda x: x.expanding().max().shift(1).dropna().iloc[-1] if len(x) > 1 else x.iloc[0],
                "low": lambda x: x.expanding().min().shift(1).dropna().iloc[-1] if len(x) > 1 else x.iloc[0],
                "close": lambda x: x.iloc[-2] if len(x) > 1 else np.nan,
                "volume": lambda x: x.iloc[:-1].sum() if len(x) > 1 else 0,
            }
            agg_dict = {
                col: (base_aggs[col] if col in base_aggs else (lambda x: x.shift(1).iloc[-1] if len(x) > 1 else np.nan))
                for col in df.columns
            }
            resampled_rows = [_safe_agg_group(grp, agg_dict) for _, grp in df.groupby(pd.Grouper(freq=self.main_timeframe))]
            df = pd.concat([r for r in resampled_rows if r is not None]) if resampled_rows else pd.DataFrame(columns=df.columns)
            df = df[~df.index.duplicated(keep="last")]
            df.replace([np.inf, -np.inf], np.nan, inplace=True); df.ffill(inplace=True); df.dropna(how="all", inplace=True)
            if self.verbose:
                print(f"[{tf}] after resample â†’ rows={len(df)}, cols={df.shape[1]}")

        # print("Safe resample finished")
        # ---------------- LOG SCALE VOLUME ----------------
        heavy_regex = r"(?:^|_)(?:volume|obv|vpt|adi|nvi|eom|vr)(?:_|$)"
        heavy_cols = df.columns[df.columns.str.contains(heavy_regex, regex=True, case=False)]
        df[heavy_cols] = np.sign(df[heavy_cols]) * np.log1p(np.abs(df[heavy_cols]))

        # ---------------- CALENDAR COLUMNS ----------------
        df.reset_index(inplace=True)
        if "time" not in df.columns:
            df.rename(columns={df.columns[0]: "time"}, inplace=True)
        df["time"] = pd.to_datetime(df["time"], errors="coerce")
        df.rename(columns={"time": f"{tf}_time"}, inplace=True)
        if df[f"{tf}_time"].notna().any():
            df[f"{prefix}hour"] = df[f"{tf}_time"].shift(1).dt.hour
            df[f"{prefix}day_of_week"] = df[f"{tf}_time"].shift(1).dt.dayofweek
            df[f"{prefix}is_weekend"] = df[f"{prefix}day_of_week"].isin([5, 6]).astype(int)
        _timedelta_to_seconds(df)

        # print("Load and process time frame finished")
        return df

    # ================= 2) FEATURE SELECTION =================
    def select_features(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        top_k: int = 300,
        n_splits: int = 3,
    ) -> List[str]:
        """Timeâ€‘series aware feature selection (Varianceâ†’Corrâ€‘filterâ†’Mutualâ€‘Info).

        * ØªÙ…Ø§Ù… NaN/Inf Ù¾ÛŒØ´ Ø§Ø² MI Ø¨Ø§ **Ù…ÛŒØ§Ù†Ù‡** Ø³ØªÙˆÙ† Ù¾Ø± Ù…ÛŒâ€ŒØ´ÙˆØ¯ ØªØ§ Ø®Ø·Ø§ÛŒ "contains NaN" Ø±ÙØ¹ Ø´ÙˆØ¯.
        """
        # print("Feature selection start ...")
        tscv = TimeSeriesSplit(n_splits=n_splits)
        pool: List[str] = []
        for tr_idx, _ in tscv.split(X):
            X_tr, y_tr = X.iloc[tr_idx], y.iloc[tr_idx]
            # 1) Variance threshold
            vt = VarianceThreshold(0.01)
            cols_var = X_tr.columns[vt.fit(X_tr).get_support()]
            if cols_var.empty:
                continue
            X_var = X_tr[cols_var]
            # 2) Correlation filter
            corr = X_var.corr().abs(); mask = np.triu(np.ones_like(corr, bool), k=1)
            upper = corr.where(mask)
            drop_cols = [c for c in upper.columns if any(upper[c] > 0.9)]
            X_corr = X_var.drop(columns=drop_cols, errors="ignore")
            if X_corr.empty:
                continue
            # ---- FILL NA BEFORE MI ----
            X_filled = X_corr.replace([np.inf, -np.inf], np.nan).fillna(X_corr.median())
            # 3) Mutual information
            X_scaled = MinMaxScaler().fit_transform(X_filled)
            mi = mutual_info_classif(X_scaled, y_tr, random_state=SEED)
            pool.extend(pd.Series(mi, index=X_corr.columns).nlargest(top_k).index.tolist())
        counts = pd.Series(pool).value_counts()
        # print("Feature selection finished")
        return counts[counts >= n_splits].index[:top_k].tolist()

    # ================= 3) READY (X, y, WINDOW) =================
    def ready(
        self,
        data: pd.DataFrame,
        window: int = 1,
        selected_features: List[str] | None = None,
        mode: str = "train",
        with_times: bool = False,
        predict_drop_last: bool = False,   # â† Ø¬Ø¯ÛŒØ¯
    ):

        close_col = f"{self.main_timeframe}_close"
        if close_col not in data.columns:
            raise ValueError(f"{close_col} missing")

        # y(t) = 1{close(t+1) > close(t)}
        y = ((data[close_col].shift(-1) - data[close_col]) > 0).astype(int)
        if mode != "train":
            y.iloc[:] = 0  # Ø¯Ø± predict ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ù‡Ù…â€ŒØªØ±Ø§Ø²ÛŒ Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø±ÛŒÙ…

        # Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
        time_cols = [c for c in data.columns if any(tok in c for tok in ["hour","day_of_week","is_weekend"])]
        feat_cols = [c for c in data.columns if c not in time_cols + [close_col]]

        # ğŸ”‘ ÙÛŒÚ†Ø±Ù‡Ø§ ÙÙ‚Ø· Ø§Ø² Ú¯Ø°Ø´ØªÙ‡ Ø³Ø§Ø®ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯: diff Ø±ÙˆÛŒ shift(1)
        df_diff = self._compute_diff(data, feat_cols, strict_cols)
        df_diff.replace([np.inf, -np.inf], np.nan, inplace=True)
        df_diff.ffill(inplace=True)
        df_diff.dropna(how="all", inplace=True)
        df_diff.reset_index(drop=True, inplace=True)

        # y Ø±Ø§ Ø¨Ø§ df_diff Ù‡Ù…â€ŒÙ‚Ø¯ Ú©Ù†
        y = y.iloc[:len(df_diff)].reset_index(drop=True)

        # âš ï¸ Ø¯Ø± PREDICT Ù‡ÛŒÚ†â€ŒÚ†ÛŒØ² Ø±Ø§ Ø­Ø°Ù Ù†Ú©Ù†Ø› Ø±Ø¯ÛŒÙ Ø¢Ø®Ø± X Ø§Ø² tâˆ’1 Ùˆ tâˆ’2 Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ Ùˆ Ù¾Ø§ÛŒØ¯Ø§Ø± Ø§Ø³Øª.
        # âŒ Ø§ÛŒÙ† Ø¨Ù„ÙˆÚ© Ø±Ø§ Ø§Ú¯Ø± Ø¯Ø§Ø±ÛŒ Ø­Ø°Ù Ú©Ù†:
        # if mode == "predict":
        #     df_diff = df_diff.iloc[:-1]  # â† Ø§ÛŒÙ† Ø¨Ø§ÛŒØ¯ Ø­Ø°Ù Ø´ÙˆØ¯

        # Ø§Ù†ØªØ®Ø§Ø¨ ÙÛŒÚ†Ø± (+ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² _tminus)
        import re as _re
        tminus_regex = _re.compile(r"_tminus\d+$")
        has_tminus = bool(selected_features and any(tminus_regex.search(f) for f in selected_features))
        if has_tminus:
            base_feats = {tminus_regex.sub("", f) for f in selected_features}
            feats = [f for f in base_feats if f in df_diff.columns]
            strict_cols = True
        else:
            if selected_features is None:
                feats = self.select_features(df_diff, y)
            elif selected_features == []:
                feats = df_diff.columns.tolist()
            else:
                feats = [f for f in selected_features if f in df_diff.columns]

        if (not strict_cols) and self.bad_cols_tf:
            bad_union = set().union(*self.bad_cols_tf.values())
            feats = [f for f in feats if f not in bad_union]

        X_f = df_diff[feats].copy()

        # Ù¾Ù†Ø¬Ø±Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
        X_f, y, feats = self._apply_window(X_f, y, feats, window, selected_features, has_tminus)

        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
        X_f.replace([np.inf, -np.inf], np.nan, inplace=True)
        X_f = X_f.fillna(X_f.median())

        # Ø²Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ù‡Ù…â€ŒØªØ±Ø§Ø² (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
        tcol = f"{self.main_timeframe}_time" if f"{self.main_timeframe}_time" in data.columns else "time"
        t_idx = pd.to_datetime(data[tcol]).reset_index(drop=True)
        if len(t_idx) > 0:
            t_idx = t_idx.iloc[1:].reset_index(drop=True)  # Ú†ÙˆÙ† diff ÛŒÚ© Ø±Ø¯ÛŒÙ Ù…ÛŒâ€ŒØ³ÙˆØ²Ø§Ù†Ø¯
        if window > 1 and len(t_idx) >= window-1:
            t_idx = t_idx.iloc[window-1:].reset_index(drop=True)

        # Ù‡Ù…â€ŒØªØ±Ø§Ø²ÛŒ Ø·ÙˆÙ„â€ŒÙ‡Ø§
        L = min(len(X_f), len(y), len(t_idx))
        X_f = X_f.iloc[:L].reset_index(drop=True)
        y   = y.iloc[:L].reset_index(drop=True)
        t_idx = t_idx.iloc[:L].reset_index(drop=True)

        # â‹ ÙÙ‚Ø· Ø¯Ø± TRAIN: Ø­Ø°Ù Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù‡Ø¯Ù Ù†Ø¯Ø§Ø±Ù†Ø¯ (close_{t+1} ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯)
        if mode == "train":
            close_col = f"{self.main_timeframe}_close"
            # y Ù…Ø¹ØªØ¨Ø± ÙˆÙ‚ØªÛŒ Ø§Ø³Øª Ú©Ù‡ close(t+1) Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯
            diff_next = data[close_col].shift(-1) - data[close_col]  # t+1 - t
            valid = diff_next.iloc[:len(df_diff)].reset_index(drop=True).notna()

            # Ù‡Ù…â€ŒØªØ±Ø§Ø²ÛŒ Ø¨Ø§ Ù¾Ù†Ø¬Ø±Ù‡â€ŒØ¨Ù†Ø¯ÛŒ (window-1 Ø±Ø¯ÛŒÙ Ø§Ø¨ØªØ¯Ø§ÛŒ X Ø­Ø°Ù Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯)
            if window > 1 and len(valid) >= (window - 1):
                valid = valid.iloc[window - 1:].reset_index(drop=True)

            # Ù‡Ù…â€ŒØ·ÙˆÙ„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ X_f
            L = min(len(valid), len(X_f))
            valid = valid.iloc[:L].astype(bool)

            # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù†Ù Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§
            X_f = X_f.loc[valid].reset_index(drop=True)
            y   = y.loc[valid].reset_index(drop=True)
            try:
                t_idx = t_idx.loc[valid].reset_index(drop=True)  # Ø§Ú¯Ø± with_times=True
            except NameError:
                pass

            self.train_columns_after_window = X_f.columns.tolist()


        price_raw = data[close_col].iloc[:len(df_diff)].reset_index(drop=True)
        if window > 1:
            price_raw = price_raw.iloc[window-1:].reset_index(drop=True)
        price_raw = price_raw.iloc[:len(X_f)].reset_index(drop=True)
        # --- REAL-Stable: Ø¨Ø¹Ø¯ Ø§Ø² Ø³Ø§Ø®Øª ÙÛŒÚ†Ø±Ù‡Ø§ØŒ Ø¢Ø®Ø±ÛŒÙ† Ø±Ø¯ÛŒÙ Ø±Ø§ Ø­Ø°Ù Ú©Ù† ØªØ§ X Ø¢Ø®Ø± = t-1 Ø¨Ø§Ø´Ø¯
        if (mode != "train") and predict_drop_last and len(X_f) >= 1:
            X_f      = X_f.iloc[:-1].reset_index(drop=True)
            y        = y.iloc[:-1].reset_index(drop=True)
            if with_times and (t_idx is not None) and (len(t_idx) >= 1):
                t_idx = t_idx.iloc[:-1].reset_index(drop=True)
            if price_raw is not None and len(price_raw) >= 1:
                price_raw = price_raw.iloc[:-1].reset_index(drop=True)


        if with_times:
            return X_f, y, feats, price_raw, t_idx
        else:
            return X_f, y, feats, price_raw

    # ================= 4) READY_INCREMENTAL =================
    def ready_incremental(
        self,
        data_window: pd.DataFrame,
        window: int = 1,
        selected_features: List[str] | None = None,
    ):
        """
        Live-safe wrapper around ``ready``.
        Ø¢Ø®Ø±ÛŒÙ† Ø¯Ùˆ Ø±Ú©ÙˆØ±Ø¯ Ø®Ø§Ù… Ù†Ú¯Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ ØªØ§ Ø¹Ù…Ù„ÛŒØ§Øª diff Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ù…Ø«Ù„ Ø­Ø§Ù„Øª
        batch Ø¨Ø§Ø´Ø¯ Ùˆ Ù‡ÛŒÚ† Ø³ØªÙˆÙ†Ù _tminus Ø§Ø² Ø¯Ø§Ø¯Ù‡ Ø¨ÛŒâ€ŒØ®Ø¨Ø± Ù†Ù…Ø§Ù†Ø¯.
        """
        if not hasattr(self, "_live_prev2"):
            # Ø§ÙˆÙ„ÛŒÙ† ÙØ±Ø§Ø®ÙˆØ§Ù†: ÙÙ‚Ø· Ø¨Ø§ÙØ± Ø±Ø§ Ù¾Ø± Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            self._live_prev2 = data_window.iloc[-2:].copy()
            return pd.DataFrame(), []

        # Ú†Ø³Ø¨Ø§Ù†Ø¯Ù† Ø¯Ùˆ Ø±Ú©ÙˆØ±Ø¯ Ù‚Ø¨Ù„ÛŒ Ø¨Ù‡ Ø§Ø¨ØªØ¯Ø§ÛŒ Ù¾Ù†Ø¬Ø±Ù‡Ù” Ø¬Ø¯ÛŒØ¯
        concat = pd.concat(
            [self._live_prev2, data_window], ignore_index=True
        )

        # Ø­Ø³Ø§Ø¨ Ø¯Ù‚ÛŒÙ‚ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ â€“ Ø¹ÛŒÙ† ready
        X_full, _, feats, _ = self.ready(
            concat,
            window=window,
            selected_features=selected_features,
            mode="predict",
        )

        # Ø¨Ø§ÙØ± Ø±Ø§ Ø¨Ø±Ø§ÛŒ ÙØ±Ø§Ø®ÙˆØ§Ù† Ø¨Ø¹Ø¯ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ú©Ù†
        self._live_prev2 = data_window.iloc[-2:].copy()

        # Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú†Ù†Ø¯ Ø±Ø¯ÛŒÙ Ø¨Ø¯Ù‡Ø¯ (Ø§Ú¯Ø± window>1) â†’ ÙÙ‚Ø· Ø¢Ø®Ø±ÛŒÙ† Ø±Ú©ÙˆØ±Ø¯
        if X_full.empty:
            return pd.DataFrame(), feats

        return X_full.tail(1).reset_index(drop=True), feats

    # ================= 5) LOAD & MERGE =================
    def load_data(self) -> pd.DataFrame:
        logging.info("[load_data] parallel load %d timeframes", len(self.filepaths))

        # ---------- 1) Ù…ÙˆØ§Ø²ÛŒ-Ø®ÙˆØ§Ù†ÛŒ Ùˆ Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ù‡Ø± ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… ----------
        dfs = Parallel(
            n_jobs=min(mp.cpu_count(), len(self.filepaths)),
            backend="loky"
        )(
            delayed(self.load_and_process_timeframe)(tf, fp)
            for tf, fp in self.filepaths.items()
        )

        # ---------- 2) Ø§Ø¯ØºØ§Ù… Ø±ÙˆÛŒ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… Ø§ØµÙ„ÛŒ ----------
        main_tf  = self.main_timeframe
        main_df  = dfs[0].set_index(f"{main_tf}_time", drop=False)
        for (tf, _), df in zip(list(self.filepaths.items())[1:], dfs[1:]):
            main_df = main_df.join(
                df.set_index(f"{tf}_time", drop=False),
                how="outer",
                rsuffix=f"_{tf}",
            )

        # ---------- 3) Ø­Ø°Ù Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Â«Ù†Ø§Ù¾Ø§ÛŒØ¯Ø§Ø±Â» Ú©Ù‡ Ù‚Ø¨Ù„Ø§Ù‹ Ø¨Ø±Ø§ÛŒ Ù‡Ø± TF Ú©Ø´Ù Ø´Ø¯Ù‡ ----------
        for tf, bad_set in self.bad_cols_tf.items():
            if not bad_set:
                continue
            cols_to_drop = [c for c in main_df.columns if c in bad_set]
            if cols_to_drop:
                main_df.drop(columns=cols_to_drop, inplace=True, errors="ignore")
                if self.verbose:
                    print(f"[DROP] Removed {len(cols_to_drop)} unstable cols from {tf}")

        # ---------- 4) Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ NaN/Inf Ùˆ ÙÙˆØ±ÙˆØ§Ø±Ø¯-Ù¾Ø±Ú©Ø±Ø¯Ù† ----------
        main_df.replace([np.inf, -np.inf], np.nan, inplace=True)
        main_df.ffill(inplace=True)
        main_df.dropna(how="all", inplace=True)

        # ---------- 5) Ø­Ø°Ù Ø³Ø·Ø±Ù‡Ø§ÛŒ ØªØ¹Ø·ÛŒÙ„Ø§Øª (Ø´Ù†Ø¨Ù‡/ÛŒÚ©Ø´Ù†Ø¨Ù‡) ----------
        if isinstance(main_df.index, pd.DatetimeIndex):
            main_df = main_df[~main_df.index.dayofweek.isin([5, 6])]
            main_df.ffill(inplace=True)

        # ---------- 6) Ù†Ù‡Ø§ÛŒÛŒâ€ŒØ³Ø§Ø²ÛŒ Ø§Ù†Ø¯ÛŒØ³ / Ø­Ø°Ù duplications ----------
        main_df.reset_index(drop=False, inplace=True)
        tcol = f"{main_tf}_time"
        if tcol not in main_df.columns:             # Ø§ÛŒÙ…Ù†ÛŒ Ø§Ú¯Ø± Ø³ØªÙˆÙ† Ø¬Ø§Ø¨Ù‡â€ŒØ¬Ø§ Ø´Ø¯
            main_df.rename(columns={main_df.columns[0]: tcol}, inplace=True)
        main_df = main_df.loc[~main_df[tcol].duplicated(keep="last")]

        logging.info("[load_data] Final shape=%s", main_df.shape)
        gc.collect()
        return main_df

    # ================= 6) OUTER INTERFACE =================
    def get_prepared_data(self, window=1, mode="train") -> Tuple[pd.DataFrame, pd.Series, List[str]]:
        merged = self.load_data()
        X, y, feats, _ = self.ready(merged, window=window, mode=mode)
        return X, y, feats



if __name__ == "__main__":
    prep = PREPARE_DATA_FOR_TRAIN(verbose=True)
    X, y, f = prep.get_prepared_data(window=1, mode="train")
    print("Shapes:", X.shape, y.shape, len(f))

