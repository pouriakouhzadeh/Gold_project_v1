#!/usr/bin/env python3
"""
Full dataâ€‘preparation pipeline for GA trainer (legacyâ€‘compatible).
----------------------------------------------------------------
* Ø¨Ø§Ø²ØªÙˆÙ„ÛŒØ¯ Ú©Ø§Ù…Ù„ ØªÙ…Ø§Ù… Ø®Ø·ÙˆØ· Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ Ù†Ø³Ø®Ù‡Ù” Ø§ØµÙ„ÛŒ (â‰ˆâ€¯630â€¯Ø³Ø·Ø±)
* Ø±ÛŒØ³Ø§Ù…Ù¾Ù„ Ø§ÛŒÙ…Ù† Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ù†Ø¯Ù„ Ù†Ø§Ù‚Øµ (first / max / min / last / sum)
* Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø§â€¯TimeSeriesSplitâ€¯+â€¯Mutualâ€¯Informationâ€¯+â€¯Ø­Ø°Ù Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ
* Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø²ÛŒ + Ù¾Ù†Ø¬Ø±Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ù†Ø¹Ø·Ø§Ùâ€ŒÙ¾Ø°ÛŒØ±
* Ø­Ø°ÙÙ ØªØ¹Ø·ÛŒÙ„Ø§Øª (Ø´Ù†Ø¨Ù‡/ÛŒÚ©Ø´Ù†Ø¨Ù‡) Ùˆ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
* Ø¨Ø°Ø± ØªØµØ§Ø¯ÙÛŒ Ø«Ø§Ø¨Øªâ€¯(2025) Ùˆ Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ Ø®Ù„Ø§ØµÙ‡
"""
from __future__ import annotations
import os
import gc
import re
from collections import defaultdict
import logging
import multiprocessing as mp
import warnings
from typing import List, Tuple
from AllIndicatorsNoLeak import AllIndicatorsNoLeak
from custotechIndicators import CustomTechIndicators
import numpy as np
import pandas as pd
from collections import defaultdict, Counter
from leakfree_indicators import LeakFreeBatchLive
from joblib import Parallel, delayed
from numba import config as numba_config
from sklearn.feature_selection import VarianceThreshold, mutual_info_classif
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import MinMaxScaler
from sklearnex import patch_sklearn
from pathlib import Path
import json
from clear_data import ClearData
from DriftBasedStartDateSuggester import DriftBasedStartDateSuggester
from custom_indicators import (
    CustomCandlestickPattern,
    CustomIchimokuIndicator,
    CustomPivotPointIndicator,
    CustomVolumeRateOfChangeIndicator,
    CustomWilliamsRIndicator,
    KSTCustomIndicator,
    VortexCustomIndicator,
)
from numba_utils import (
    numba_kurtosis,
    numba_last_local_max_idx,
    numba_last_local_min_idx,
    numba_median,
    numba_skew,
    numba_up_count,
)
from time_utils import TimeColumnFixer as TFix
from stable_extra_features import add_stable_extra_features
from strong_feature_selector import StrongFeatureSelector  


patch_sklearn(verbose=False)

# ---------------- LOGGING & WARNINGS ----------------
logging.getLogger("sklearnex").setLevel(logging.WARNING)
logging.getLogger("sklearn").setLevel(logging.WARNING)
logging.getLogger("daal4py").setLevel(logging.WARNING)
logging.getLogger("numba").setLevel(logging.CRITICAL)
numba_config.LOG_LEVEL = "CRITICAL"

warnings.filterwarnings("ignore")
logging.basicConfig(level=logging.INFO, filename="genetic_algorithm.log", format="%(asctime)s %(levelname)s:%(message)s")

# ---------------- REPRODUCIBILITY ----------------
SEED = 2025
np.random.seed(SEED)


# ---------------- BLACKLIST ----------------

# Ø¯Ø± Ø¨Ø§Ù„Ø§ÛŒ prepare_data_for_train.py (Ø¬Ø§ÛŒÛŒ Ú©Ù‡ helper Ù‚Ø¨Ù„ÛŒ Ù‡Ø³Øª)

def _load_feature_blacklist(parity_path: str = "features_parity_summary.csv") -> set[str]:
    """
    Ù„ÛŒØ³Øª ÙÛŒÚ†Ø±Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù†Ø¨Ø§ÛŒØ¯ Ø¯Ø± Ø¢Ù…ÙˆØ²Ø´/Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆÙ†Ø¯.
    Ø®Ø±ÙˆØ¬ÛŒ: Ù†Ø§Ù… ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ *Ù¾Ø§ÛŒÙ‡* (Ø¨Ø¯ÙˆÙ† suffix Ù‡Ø§ÛŒ _tminusN).

    Ù…Ù†Ø§Ø¨Ø¹ÛŒ Ú©Ù‡ Ø¨Ø§ Ù‡Ù… Ø§Ø¯ØºØ§Ù… Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯:
      1) feature_blacklist.txt
         - Ù‡Ø± Ø®Ø· ÛŒÚ© Ù†Ø§Ù… ÙÛŒÚ†Ø± Ù¾Ø§ÛŒÙ‡Ø› Ù…Ø«Ø§Ù„: '30T_rsi_14'
      2) feature_blacklist.json
         - JSON array Ø§Ø² Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§
      3) features_compare_summary.csv
         - Ø®Ø±ÙˆØ¬ÛŒ compare_feature_feeds / Ø§Ø³Ú©Ø±ÛŒÙ¾Øªâ€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ
      4) features_parity_summary.csv
         - Ø®Ø±ÙˆØ¬ÛŒ batch_vs_live_feature_parity.py
           (Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ feature Ùˆ ratio_diff / n_diff)
    """

    bl: set[str] = set()

    def _add_base(name: str) -> None:
        """Ù†Ø§Ù… ÙÛŒÚ†Ø± (Ø¨Ø§ ÛŒØ§ Ø¨Ø¯ÙˆÙ† _tminusN) Ø±Ø§ Ø¨Ù‡ Ù†Ø§Ù… Ù¾Ø§ÛŒÙ‡ ØªØ¨Ø¯ÛŒÙ„ Ùˆ Ø¯Ø± Ø¨Ù„Ø§Ú©â€ŒÙ„ÛŒØ³Øª Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
        name = (name or "").strip()
        if not name:
            return
        # Ø­Ø°Ù suffix Ù…Ø«Ù„ _tminus0 ÛŒØ§ _tminus12
        base = re.sub(r"_tminus\d+$", "", name)
        if base:
            bl.add(base)

    # Û±) ÙØ§ÛŒÙ„ Ù…ØªÙ†ÛŒ Ø¯Ø³ØªÛŒ (feature_blacklist.txt)
    txt_path = Path("feature_blacklist.txt")
    if txt_path.exists():
        for ln in txt_path.read_text(encoding="utf-8").splitlines():
            ln = ln.strip()
            if ln:
                _add_base(ln)

    # Û²) ÙØ§ÛŒÙ„ JSON Ø¯Ø³ØªÛŒ (feature_blacklist.json)
    json_path = Path("feature_blacklist.json")
    if json_path.exists():
        try:
            arr = json.loads(json_path.read_text(encoding="utf-8"))
            if isinstance(arr, (list, tuple)):
                for name in arr:
                    if isinstance(name, str) and name.strip():
                        _add_base(name)
        except Exception:
            # Ø§Ú¯Ø± JSON Ø®Ø±Ø§Ø¨ Ø¨ÙˆØ¯ØŒ Ú©Ù„ ÙˆØ±ÙˆØ¯ÛŒ Ø±Ø§ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…
            pass

    # Û³) Ú¯Ø²Ø§Ø±Ø´ Ù‚Ø¯ÛŒÙ…ÛŒ features_compare_summary.csv
    compare_path = Path("features_compare_summary.csv")
    if compare_path.exists():
        try:
            dfc = pd.read_csv(compare_path)
            if "feature" in dfc.columns:
                if "mismatch_cnt" in dfc.columns:
                    mask = dfc["mismatch_cnt"] > 0
                    col = dfc.loc[mask, "feature"].astype(str)
                else:
                    # Ø§Ú¯Ø± Ø³ØªÙˆÙ† mismatch_cnt Ù†Ø¨ÙˆØ¯ØŒ Ú©Ù„ feature Ù‡Ø§ Ø±Ø§ Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…
                    col = dfc["feature"].astype(str)

                for f in col:
                    _add_base(f)
        except Exception:
            pass

    # Û´) Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø¯ÛŒØ¯ features_parity_summary.csv (batch vs live)
    parity_path_obj = Path(parity_path)
    if parity_path_obj.exists():
        try:
            dfp = pd.read_csv(parity_path_obj)
            if "feature" in dfp.columns:
                if "ratio_diff" in dfp.columns:
                    # Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÛŒ Ù‚Ø§Ø¨Ù„ ØªÙ†Ø¸ÛŒÙ…Ø› ÙØ¹Ù„Ø§Ù‹ 1e-3
                    mask = dfp["ratio_diff"] > 1e-3
                    col = dfp.loc[mask, "feature"].astype(str)
                else:
                    # Ø§Ú¯Ø± ratio_diff Ù†Ø¨ÙˆØ¯ØŒ Ø§Ø² n_diff Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
                    if "n_diff" in dfp.columns:
                        mask = dfp["n_diff"] > 0
                        col = dfp.loc[mask, "feature"].astype(str)
                    else:
                        # Ø¢Ø®Ø±ÛŒÙ† fallback: Ù‡Ù…Ù‡â€ŒÛŒ feature Ù‡Ø§
                        col = dfp["feature"].astype(str)

                for f in col:
                    _add_base(f)
        except Exception:
            pass

    return bl

# ---------------- HELPERS ----------------

def _timedelta_to_seconds(df: pd.DataFrame):
    for col in df.columns:
        if pd.api.types.is_timedelta64_dtype(df[col]):
            df[col] = df[col].dt.total_seconds()

# ---------------- SAFE RESAMPLE ----------------

def _safe_agg_group(key: pd.Timestamp, grp: pd.DataFrame, agg_dict: dict[str, callable]):
    if len(grp) >= 2:
        out = grp.iloc[:-1].agg(agg_dict)
        out.name = key                # Ø§Ù†Ø¯ÛŒØ³ = Ú©Ù„ÛŒØ¯ Ú¯Ø±ÙˆÙ‡ (Ù„Ø¨Ù‡Ù” Ú†Ù¾ Ø¨Ø§ ÙØ±Ú©Ø§Ù†Ø³ 30T)
        return out.to_frame().T
    if len(grp) == 1:
        g = grp.iloc[[0]].copy()
        g.index = pd.DatetimeIndex([key])
        return g
    return None

# ---------------- MAIN CLASS ----------------
class PREPARE_DATA_FOR_TRAIN:
        # ------------------------------------------------------------------
    # LIVE incremental internal state  (keep last two raw rows)
    # ------------------------------------------------------------------
    _live_prev2: pd.DataFrame | None = None       # Ø¢Ø®Ø±ÛŒÙ† Ø¯Ùˆ Ø±Ø¯ÛŒÙ Ø®Ø§Ù…
    _live_prev_time: pd.Timestamp | None = None   # ÙÙ‚Ø· Ø§Ú¯Ø± Ø®ÙˆØ§Ø³ØªÛŒ ØªØ±ØªÛŒØ¨ Ø±Ø§ Ú†Ú© Ú©Ù†ÛŒ

        # --------- NEW ---------
    bad_cols_tf: dict[str, set[str]] = defaultdict(set)   # {"30T": {"colA", ...}, "1H": {...}}
    allow_regex = re.compile(r"(?:is_weekend|day_of_week|hour)$", re.I)
        # ---------- NEW: helpers to unify batch & live -----------------
    def _compute_diff(self, data: pd.DataFrame,
                      feat_cols: list[str],
                      strict_cols: bool) -> pd.DataFrame:
        """
        ÙˆØ§Ø­Ø¯ Ù…Ø±Ú©Ø²ÛŒ diff/shift + Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒØ› Ù‡Ù…Ù‡ Ø¬Ø§ ÙÙ‚Ø· Ø§ÛŒÙ† Ø±Ø§ ØµØ¯Ø§ Ù…ÛŒâ€ŒØ²Ù†ÛŒÙ….
        """
        df = data[feat_cols].shift(1).diff()
        _timedelta_to_seconds(df)           # ØªØ¨Ø¯ÛŒÙ„ timedelta Ø¨Ù‡ Ø«Ø§Ù†ÛŒÙ‡

        if not strict_cols and self.bad_cols_tf:
            bad_union = set().union(*self.bad_cols_tf.values())
            df.drop(columns=[c for c in bad_union if c in df.columns],
                    inplace=True, errors="ignore")

        # Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ ØµÙØ± (ØºÛŒØ±Ø¨Ø§ÛŒÙ†Ø±ÛŒ)
        is_bin = (df.nunique() <= 2).to_dict()
        zero_like = [c for c in df.columns
                    if df[c].abs().max() < 1e-12 and not is_bin.get(c, False)]
        if not strict_cols:                       # â† ÙÙ‚Ø· ÙˆÙ‚ØªÛŒ strict Ù†ÛŒØ³Øª Ø­Ø°Ù Ú©Ù†
            df.drop(columns=zero_like, inplace=True, errors="ignore")


        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        df.ffill(inplace=True)
        df.dropna(how="all", inplace=True)
        df.reset_index(drop=True, inplace=True)
        return df

    def _apply_window(self, X_f: pd.DataFrame, y: pd.Series,
                      feats: list[str], window: int,
                      selected_features: list[str]|None,
                      has_tminus: bool):
        """
        ØªÙˆÙ„ÛŒØ¯ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ _tminus ÙÙ‚Ø· ÛŒÚ©â€ŒØ¬Ø§ØŒ Ø¨Ø±Ø§ÛŒ batch Ùˆ live.
        """
        if window <= 1:
            return X_f, y, feats

        if len(X_f) < window:
            logging.warning("Not enough rows for window=%d", window)
            return pd.DataFrame(), pd.Series(dtype=int), feats

        stacked = np.concatenate(
            [X_f.shift(i).iloc[window-1:].values for i in range(window)],
            axis=1
        )
        X_f = pd.DataFrame(
            stacked,
            columns=[f"{c}_tminus{i}" for i in range(window) for c in feats]
        )
        y = y.iloc[window-1:].reset_index(drop=True)

        if has_tminus and selected_features:
            X_f = X_f[[c for c in selected_features if c in X_f.columns]]

        return X_f, y, feats
    # ---------- END NEW ---------------------------------------------------

    def _detect_bad_cols_tf(
        self,
        df: pd.DataFrame,
        tf: str,
        *,
        windows: tuple[int, ...] = (1,2,3,4,5,6,9,12,20,24,30,34),
        stride: int = 75,
        ratio_thr: float = 0.12,     # â† Ø­ØªÛŒ 12 Ùª Ø®Ø·Ø§ Ú©Ø§ÙÛŒ Ø§Ø³Øª
        min_fail:  int   = 8,        # â† ÛŒØ§ Ø­Ø¯Ø§Ù‚Ù„ 8 Ø¨Ø§Ø± Ø®Ø±Ø§Ø¨ÛŒ
    ) -> None:
        """
        â€¢ bad_zero_nan  : ØµÙØ±/NaN Ø¯Ø± Ø¢Ø®Ø± Ù¾Ù†Ø¬Ø±Ù‡
        â€¢ bad_forward   : ØªØºÛŒÛŒØ± Ø±Ø¯ÛŒÙ Ù…Ø§Ù‚Ø¨Ù„â€ŒØ¢Ø®Ø± Ù¾Ø³ Ø§Ø² ÙˆØ±ÙˆØ¯ Ø±Ú©ÙˆØ±Ø¯ Ø¬Ø¯ÛŒØ¯
        Ø³ØªÙˆÙ† Ø­Ø°Ù Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø§Ú¯Ø±  (fails / tests  >= ratio_thr)  Â«ÛŒØ§Â»  fails >= min_fail
        """

        # â”€â”€â”€ Ø´Ù…Ø§Ø±Ù†Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³ØªÙˆÙ†ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        zero_nan_fail   : Counter[str] = Counter()
        forward_fail    : Counter[str] = Counter()
        zero_nan_tests  : Counter[str] = Counter()
        forward_tests   : Counter[str] = Counter()

        n_segments = 8
        seg_len    = len(df) // n_segments or len(df)

        for seg_idx in range(n_segments):
            seg = df.iloc[seg_idx*seg_len : (seg_idx+1)*seg_len]

            for win in windows:
                if len(seg) <= win:  # Ù¾Ù†Ø¬Ø±Ù‡â€Œ Ø¬Ø§ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯
                    continue

                for start in range(0, len(seg) - win, stride):
                    # â”€â”€ (1) 0 / NaN Ø¯Ø± Ø¢Ø®Ø± Ù¾Ù†Ø¬Ø±Ù‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    last = seg.iloc[start+win-1]
                    bad  = last.isna() | np.isclose(last, 0.0, atol=1e-12)

                    for col in last.index:            # ØªØ³Øª Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø³ØªÙˆÙ†
                        zero_nan_tests[col] += 1
                        if bad[col]:
                            zero_nan_fail[col] += 1

                    # â”€â”€ (2) forward-looking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    if start + win >= len(seg):
                        continue                      # Ø±Ú©ÙˆØ±Ø¯ Ø§Ø¶Ø§ÙÙ‡ Ù†Ø¯Ø§Ø±ÛŒÙ…

                    sub1 = seg.iloc[start:start+win]
                    sub2 = seg.iloc[start:start+win+1]

                    pen_old = sub1.iloc[-1]
                    pen_new = sub2.iloc[-2]

                    changed = (
                        pen_old.notna()
                        & pen_new.notna()
                        & (~np.isclose(pen_old, pen_new,
                                    rtol=1e-6, atol=1e-12))
                    )

                    for col in pen_old.index:
                        forward_tests[col] += 1
                        if changed[col]:
                            forward_fail[col] += 1

        # â”€â”€â”€ ØªØµÙ…ÛŒÙ… Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø³ØªÙˆÙ† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        bad_cols: set[str] = set()

        for col in zero_nan_tests:
            # Ù†Ø³Ø¨ØªÙâ€Œ Ø®Ø·Ø§
            r = zero_nan_fail[col] / zero_nan_tests[col]
            if (r >= ratio_thr) or (zero_nan_fail[col] >= min_fail):
                bad_cols.add(col)

        for col in forward_tests:
            r = forward_fail[col] / forward_tests[col]
            if (r >= ratio_thr) or (forward_fail[col] >= min_fail):
                bad_cols.add(col)

        # â”€â”€â”€ Ø­Ø°Ù Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ø§Ø² (binary / ØªÙ‚ÙˆÛŒÙ…ÛŒ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        bad_cols = {c for c in bad_cols if not self.allow_regex.search(c)}

        # â”€â”€â”€ Ø¨Ù‡ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ú©Ù„Ø§Ø³ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self.bad_cols_tf[tf].update(bad_cols)

        if self.verbose:
            z_bad = len({c for c in bad_cols if c in zero_nan_fail})
            f_bad = len({c for c in bad_cols if c in forward_fail})
            print(f"[DETECT-{tf}] â†‘{len(bad_cols)} cols  "
                f"(0/NaN â‰¥{ratio_thr:.0%} or â‰¥{min_fail} â†’{z_bad}, "
                f"fwd â‰¥{ratio_thr:.0%} or â‰¥{min_fail} â†’{f_bad})")

    def __init__(self, filepaths: dict[str, str] | None = None, main_timeframe="30T",
                verbose=True, fast_mode: bool = False, strict_disk_feed: bool = False):
        self.main_timeframe = main_timeframe
        self.verbose = verbose
        self.fast_mode = bool(fast_mode)
        self.strict_disk_feed = bool(strict_disk_feed)
        self.train_columns_after_window: List[str] = []

        # â¬…ï¸ Ø¯ÛŒÙØ§Ù„Øª Ø§Ù…Ù† Ø¨Ø±Ø§ÛŒ Ù…Ø³ÛŒØ±Ù‡Ø§
        if filepaths is None:
            base = os.environ.get("BASE_DATA_DIR", ".")
            symbol = os.environ.get("SYMBOL", "XAUUSD")
            self.filepaths = {
                "30T": f"{base}/{symbol}_30T.csv",
                "15T": f"{base}/{symbol}_15T.csv",
                "5T":  f"{base}/{symbol}_5T.csv",
                "1H":  f"{base}/{symbol}_1H.csv",
            }
        else:
            self.filepaths = filepaths

        # ÙÙ‚Ø· Ø¯Ø± Ø­Ø§Ù„Øª Ù…Ø¹Ù…ÙˆÙ„ (Train) drift-scan Ø´ÙˆØ¯Ø› Ø¯Ø± fast_mode Ø®Ø§Ù…ÙˆØ´
        self.shared_start_date = None
        if (not fast_mode) and (not strict_disk_feed):
            self.drift_finder = DriftBasedStartDateSuggester(self.filepaths)
            self.shared_start_date = self.drift_finder.find_shared_start_date()

            if verbose:
                print(f"ğŸ“… Shared drift-aware training start date: {self.shared_start_date}")

        if verbose:
            print("[PREP] Initialised for", main_timeframe)
        logging.info("[INIT] main_timeframe=%s", self.main_timeframe)

        # ÙÙ‚Ø· ÙˆÙ‚ØªÛŒ drift-scan Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ØŒ Ø¢Ù† Ø±Ø§ Ú†Ø§Ù¾ Ú©Ù†
        if (not self.fast_mode) and (self.shared_start_date is not None):
            print(f"ğŸ“… Shared drift-aware training start date: {self.shared_start_date}")

    # ---------------- EXTRA FEATURES (stable) ----------------
    def _windows_for_tf(self, tf: str) -> tuple[int, ...]:
        """
        Ù†Ú¯Ø§Ø´Øª Ù¾Ù†Ø¬Ø±Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø§Ø²Ø§ÛŒ Ù‡Ø± TF (Ù‚Ø§Ø¨Ù„ ØªÙ†Ø¸ÛŒÙ…).
        """
        if tf in ("5T",):
            return (8, 16, 32)
        if tf in ("15T",):
            return (6, 12, 24)
        if tf in ("30T",):
            return (5, 10, 20)
        if tf in ("1H", "60T"):
            return (4, 8, 16)
        # Ù¾ÛŒØ´â€ŒÙØ±Ø¶
        return (5, 10, 20)

    def add_extra_features(self, df: pd.DataFrame, tf: str) -> pd.DataFrame:
        """
        Ø§Ø¶Ø§ÙÙ‡â€ŒÚ©Ø±Ø¯Ù† ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ù¾Ø§ÛŒØ¯Ø§Ø± Ø¨Ù‡ Â«Ù‡Ù…Ù‡â€ŒÛŒ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§Â».
        (Ù‡ÛŒÚ† look-forward Ù†Ø¯Ø§Ø±Ø¯ Ùˆ Ø³Ø·Ø± Ø¢Ø®Ø± Ù†Ø§Ù¾Ø§ÛŒØ¯Ø§Ø± ØªÙˆÙ„ÛŒØ¯ Ù†Ù…ÛŒâ€ŒÚ©Ù†Ø¯)
        """
        wins = self._windows_for_tf(tf)
        # Ø§Ú¯Ø± Ø®ÙˆØ§Ø³ØªÛŒ Ù„Ø§Ú¯ Ø¨Ú¯ÛŒØ±ÛŒ:
        # if self.verbose: print(f"[{tf}] add_stable_extra_features windows={wins}")
        return add_stable_extra_features(df, tf=tf, windows=wins, use_log_price=True)

        # ================= 1) LOAD & FEATURE ENGINEER =================
    def load_and_process_timeframe(self, tf: str, filepath: str) -> pd.DataFrame:
        # print("Load and process time frame start ...")
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"[{tf}] Data file not found: {os.path.abspath(filepath)}")
        df = ClearData().clean(pd.read_csv(filepath))
        if "time" not in df.columns:
            raise ValueError("'time' column missing in CSV")
        df["time"] = pd.to_datetime(df["time"], errors="coerce")
        df.dropna(subset=["time"], inplace=True)
        df.sort_values("time", inplace=True)

        if self.shared_start_date and (not self.strict_disk_feed):
            df = df[df["time"] >= self.shared_start_date]
            print(f"[{tf}] â³ Trimmed data from {self.shared_start_date.date()}")

        df.set_index("time", inplace=True)
        if self.verbose:
            print(f"len df {tf} = {len(df)}")
        prefix = f"{tf}_"

        # ---------------- SIMPLE ROLLING ----------------
        # df[f"{prefix}ma20"] = df["close"].rolling(20, 1).mean()
        # df[f"{prefix}ma50"] = df["close"].rolling(50, 1).mean()
        df[f"{prefix}ma_volume20"] = df["volume"].rolling(20, 1).mean()
        # df[f"{prefix}return_difference"] = df["close"].diff()
        # df[f"{prefix}roc"] = df["close"].pct_change() * 100
        df[f"{prefix}rolling_mean_20"] = df["close"].rolling(20, 1).mean()
        df[f"{prefix}rolling_std_20"] = df["close"].rolling(20, 1).std()
        df[f"{prefix}rolling_skew_20"] = df["close"].rolling(20, 1).apply(numba_skew, raw=True)
        df[f"{prefix}rolling_kurt_20"] = df["close"].rolling(20, 1).apply(numba_kurtosis, raw=True)
        df[f"{prefix}rolling_median_20"] = df["close"].rolling(20, 1).apply(numba_median, raw=True)
        df[f"{prefix}rolling_up_count_20"] = df["close"].rolling(20, 1).apply(numba_up_count, raw=True)

        # ---------------- TA FEATURES ----------------
        ta_builder = AllIndicatorsNoLeak(
            df.copy(),           # you can still pass a copy if you like
            o="open", h="high", l="low", c="close", v="volume",
            prefix=prefix        # same prefix you used before
        )
        df = ta_builder.add_features(inplace=True) 
        df = df.loc[:, ~df.columns.duplicated()]
        
        # ---------- LEAKâ€‘FREE replacements for KCP/DCP/BBP/CCI/FI/OBV/ADI/Stochâ€‘RSI/Pivot â€¦ ----------
        safe_ind = LeakFreeBatchLive(df, prefix=prefix,
                                    o="open", h="high", l="low", c="close", v="volume")
        df = pd.concat([df, safe_ind.build()], axis=1)
        df = df.loc[:, ~df.columns.duplicated()]  
        
        # ---------------- MANUAL & CUSTOM INDICATORS ----------------
        # df: Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø®Ø§Ù…ØŒ prefix Ù‡Ù…Ø§Ù† Ù…ØªØºÛŒØ± Ù‚Ø¨Ù„ÛŒ
        ind = CustomTechIndicators(df, prefix=prefix,
                                o="open", h="high", l="low", c="close", v="volume")
        df = ind.add_features(inplace=True)   # Ù‡Ù…Ù‡Ù” Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ø§ ÛŒÚ© ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ
        df = df.loc[:, ~df.columns.duplicated()]

        # ---- CUSTOM INDICATORS ----
        kst = KSTCustomIndicator(df["close"], 10, 15, 20, 30, 10, 10, 10, 15, 9, fillna=True)
        df[f"{prefix}kst_main"] = kst.kst(); df[f"{prefix}kst_signal"] = kst.kst_signal(); df[f"{prefix}kst_diff"] = kst.kst_diff()
        vtx = VortexCustomIndicator(df["high"], df["low"], df["close"], 14, fillna=True)
        df[f"{prefix}vortex_pos"] = vtx.vortex_pos(); df[f"{prefix}vortex_neg"] = vtx.vortex_neg()
        ichi = CustomIchimokuIndicator(df["high"], df["low"], df["close"], 9, 26, 52)
        df[f"{prefix}ichimoku_conversion_line"] = ichi.ichimoku_conversion_line()
        df[f"{prefix}ichimoku_base_line"] = ichi.ichimoku_base_line()
        df[f"{prefix}ichimoku_a"] = (df[f"{prefix}ichimoku_conversion_line"] + df[f"{prefix}ichimoku_base_line"])/2
        df[f"{prefix}ichimoku_b"] = (df["high"].rolling(52).max() + df["low"].rolling(52).min())/2
        df[f"{prefix}williams_r"] = CustomWilliamsRIndicator(df["high"], df["low"], df["close"], 14).williams_r()
        df[f"{prefix}vroc"] = CustomVolumeRateOfChangeIndicator(df["volume"], 20).volume_rate_of_change()
        piv = CustomPivotPointIndicator(df["high"], df["low"], df["close"], 5)
        # df[f"{prefix}pivot"] = piv.pivot(); df[f"{prefix}support_1"] = piv.support_1(); df[f"{prefix}support_2"] = piv.support_2(); df[f"{prefix}resistance_1"] = piv.resistance_1(); df[f"{prefix}resistance_2"] = piv.resistance_2()
        candle = CustomCandlestickPattern(df["open"], df["high"], df["low"], df["close"])
        df[f"{prefix}engulfing"] = candle.engulfing(); df[f"{prefix}doji"] = candle.doji()
        ha_close = (df["open"] + df["high"] + df["low"] + df["close"]) / 4; ha_open = ha_close.shift(1).ffill()
        df[f"{prefix}heikin_ashi_open"] = ha_open; df[f"{prefix}heikin_ashi_close"] = ha_close
        df[f"{prefix}range_close_ratio"] = (df["high"] - df["low"]) / (df["close"] + 1e-9)
        df[f"{prefix}bull_power"] = (df["close"] - df["low"]) / ((df["high"] - df["low"]) + 1e-9)
        df[f"{prefix}bars_from_local_max_20"] = df["close"].rolling(20, 1).apply(numba_last_local_max_idx, raw=True)
        df[f"{prefix}bars_from_local_min_20"] = df["close"].rolling(20, 1).apply(numba_last_local_min_idx, raw=True)
        df[f"{prefix}rsi_macd"] = df[f"{prefix}rsi_14"] * df[f"{prefix}macd"]
        # df[f"{prefix}ma20_ma50_ratio"] = df[f"{prefix}ma20"] / (df[f"{prefix}ma50"] + 1e-9)
        # ---------- Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…Ù Ù¾ÛŒØ´ÙˆÙ†Ø¯Ø¯Ø§Ø± ----------
        for base_col in ("open", "high", "low", "close", "volume"):
            pref_col = f"{prefix}{base_col}"
            if pref_col not in df.columns:
                df[pref_col] = df[base_col]

        df.replace([np.inf, -np.inf], np.nan, inplace=True); df.ffill(inplace=True); df.dropna(how="all", inplace=True)
        
        # ---- NEW: detect bad cols for this timeframe BEFORE resample ----
        if not getattr(self, "fast_mode", False):
            self._detect_bad_cols_tf(df, tf)

        # ---------------- SAFE RESAMPLE ----------------
        # print("Safe resample start ...")
        if tf != self.main_timeframe:
            base_aggs = {
                "open": lambda x: x.iloc[0] if not x.empty else np.nan,
                "high": lambda x: x.expanding().max().shift(1).dropna().iloc[-1] if len(x) > 1 else x.iloc[0],
                "low": lambda x: x.expanding().min().shift(1).dropna().iloc[-1] if len(x) > 1 else x.iloc[0],
                "close": lambda x: x.iloc[-2] if len(x) > 1 else np.nan,
                "volume": lambda x: x.iloc[:-1].sum() if len(x) > 1 else 0,
            }
            agg_dict = {
                col: (base_aggs[col] if col in base_aggs else (lambda x: x.shift(1).iloc[-1] if len(x) > 1 else np.nan))
                for col in df.columns
            }
            resampled_rows = [
            _safe_agg_group(key, grp, agg_dict)
            for key, grp in df.groupby(pd.Grouper(freq=self.main_timeframe))
            ]
            df = pd.concat([r for r in resampled_rows if r is not None]) if resampled_rows else pd.DataFrame(columns=df.columns)
            df = df[~df.index.duplicated(keep="last")]
            df.replace([np.inf, -np.inf], np.nan, inplace=True); df.ffill(inplace=True); df.dropna(how="all", inplace=True)
            if self.verbose:
                print(f"[{tf}] after resample â†’ rows={len(df)}, cols={df.shape[1]}")

        # print("Safe resample finished")
        # ---------------- LOG SCALE VOLUME ----------------
        heavy_regex = r"(?:^|_)(?:volume|obv|vpt|adi|nvi|eom|vr)(?:_|$)"
        heavy_cols = df.columns[df.columns.str.contains(heavy_regex, regex=True, case=False)]
        df[heavy_cols] = np.sign(df[heavy_cols]) * np.log1p(np.abs(df[heavy_cols]))

        # ---------------- add extra stable features (for ALL TFs) ---------------
        df = self.add_extra_features(df, tf=tf)

        # ---------------- CALENDAR COLUMNS ----------------
        df.reset_index(inplace=True)
        if "time" not in df.columns:
            df.rename(columns={df.columns[0]: "time"}, inplace=True)
        df["time"] = pd.to_datetime(df["time"], errors="coerce")
        df.rename(columns={"time": f"{tf}_time"}, inplace=True)
        if df[f"{tf}_time"].notna().any():
            df[f"{prefix}hour"] = df[f"{tf}_time"].shift(1).dt.hour
            df[f"{prefix}day_of_week"] = df[f"{tf}_time"].shift(1).dt.dayofweek
            df[f"{prefix}is_weekend"] = df[f"{prefix}day_of_week"].isin([5, 6]).astype(int)
        _timedelta_to_seconds(df)

        # print("Load and process time frame finished")
        return df

    def select_features(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        top_k: int = 300,
        n_splits: int = 3,
    ) -> List[str]:
        """Time series aware feature selection (Varianceâ†’Corr filterâ†’Mutual Info).

        * ÙÙ‚Ø· Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ (datetime / object Ø­Ø°Ù Ù…ÛŒâ€ŒØ´ÙˆØ¯).
        * NaN / Inf Ù‚Ø¨Ù„ Ø§Ø² VarianceThreshold Ùˆ MI Ø¨Ø§ Ù…ÛŒØ§Ù†Ù‡â€ŒÛŒ Ø³ØªÙˆÙ† Ù¾Ø± Ù…ÛŒâ€ŒØ´ÙˆØ¯.
        """
        logger = logging.getLogger(__name__)

        # --- ÙÙ‚Ø· Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ ---
        num_cols = X.select_dtypes(include=[np.number]).columns
        dropped = [c for c in X.columns if c not in num_cols]
        if dropped:
            logger.info("[select_features] dropping non-numeric columns: %s", dropped)
        if len(num_cols) == 0:
            return []

        X = X[num_cols].copy()

        tscv = TimeSeriesSplit(n_splits=n_splits)
        pool: List[str] = []

        for tr_idx, _ in tscv.split(X):
            X_tr = X.iloc[tr_idx].copy()
            y_tr = y.iloc[tr_idx]

            # --- Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ NaN / Inf Ø¯Ø± Ø§ÛŒÙ† ÙÙˆÙ„Ø¯ ---
            X_tr.replace([np.inf, -np.inf], np.nan, inplace=True)
            if X_tr.isna().any().any():
                X_tr.fillna(X_tr.median(), inplace=True)

            if X_tr.shape[1] == 0:
                continue

            # 1) Variance threshold
            vt = VarianceThreshold(0.01)
            try:
                vt.fit(X_tr)
            except Exception as e:
                logger.warning("[select_features] VarianceThreshold failed: %s", e)
                continue

            cols_var = X_tr.columns[vt.get_support()]
            if len(cols_var) == 0:
                continue
            X_var = X_tr[cols_var]

            # 2) Correlation filter
            corr = X_var.corr().abs()
            if corr.empty:
                continue

            mask_corr = np.triu(np.ones_like(corr, bool), k=1)
            upper = corr.where(mask_corr)
            drop_cols = [c for c in upper.columns if any(upper[c] > 0.9)]
            X_corr = X_var.drop(columns=drop_cols, errors="ignore")
            if X_corr.empty:
                continue

            # 3) Mutual information (Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÛŒ scale Ø´Ø¯Ù‡ Ùˆ Ø¨Ø¯ÙˆÙ† NaN)
            X_filled = X_corr.replace([np.inf, -np.inf], np.nan)
            if X_filled.isna().any().any():
                X_filled = X_filled.fillna(X_filled.median())

            X_scaled = MinMaxScaler().fit_transform(X_filled)

            mask_y = pd.Series(y_tr).notna().to_numpy()
            if mask_y.sum() < 2:
                continue

            y_arr = pd.Series(y_tr).loc[mask_y].astype(np.int64).to_numpy(copy=False)
            mi = mutual_info_classif(X_scaled[mask_y], y_arr, random_state=SEED)

            pool.extend(
                pd.Series(mi, index=X_corr.columns)
                .nlargest(top_k)
                .index
                .tolist()
            )

        if not pool:
            return []

        counts = pd.Series(pool).value_counts()
        return counts[counts >= n_splits].index[:top_k].tolist()

    # ================= 3) READY (X, y, WINDOW) =================
    def ready(
            self,
            data: pd.DataFrame,
            window: int = 1,
            selected_features: List[str] | None = None,
            mode: str = "train",
            with_times: bool = False,
            predict_drop_last: bool = False,
            train_drop_last: bool = False,
            apply_strong_fs: bool = False,
            strong_fs_max_features: int = 300,
        ):
        """
        Ù†Ø³Ø®Ù‡â€ŒÛŒ Ù‡Ù…â€ŒÙ…Ø¹Ù†Ø§ Ø¨Ø§ Â«Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù†Ø¯Ù„ Ø¨Ø¹Ø¯ÛŒÂ»:

        - Ø¯Ø± TRAIN:
            * Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù†Ø¯Ù„ Ø¨Ø§ Ø²Ù…Ø§Ù† tØŒ Ø¨Ø±Ú†Ø³Ø¨ y(t) = 1{ close(t+1) > close(t) }.
            * Ø¢Ø®Ø±ÛŒÙ† Ú©Ù†Ø¯Ù„ (Ú©Ù‡ close(t+1) Ù†Ø¯Ø§Ø±Ø¯) Ø¨Ù‡ Ø·ÙˆØ± Ø®ÙˆØ¯Ú©Ø§Ø± Ø­Ø°Ù Ù…ÛŒâ€ŒØ´ÙˆØ¯.
        - Ø¯Ø± PREDICT:
            * Ø§ØµÙ„Ø§Ù‹ ØªØ§Ø±Ú¯Øª ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯Ø› ÙÙ‚Ø· ÙÛŒÚ†Ø±Ù‡Ø§ Ø³Ø§Ø®ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
            * Ø¢Ø®Ø±ÛŒÙ† Ú©Ù†Ø¯Ù„ (t_now) Ù‡Ù… Ù†Ú¯Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ ØªØ§ Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ø¢Ù† tâ†’t+1 Ø±Ø§ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù†Ø¯.
        - Ù¾Ù†Ø¬Ø±Ù‡â€ŒØ¨Ù†Ø¯ÛŒ (window>1):
            * Ø³Ø·Ø± Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø§ Ø²Ù…Ø§Ù† t Ù‡Ù…Ø§Ù† Ø³ØªÙˆÙ†ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø§Ø² Ú©Ù†Ø¯Ù„â€ŒÙ‡Ø§ÛŒ t, t-1, ..., t-window+1 Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ØŒ
              Ùˆ Ù‡Ù…Ú†Ù†Ø§Ù† y(t) = 1{ close(t+1) > close(t) } Ø±Ø§ Ù‡Ø¯Ù Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.
        """

        close_col = f"{self.main_timeframe}_close"
        tcol = (
            f"{self.main_timeframe}_time"
            if f"{self.main_timeframe}_time" in data.columns
            else "time"
        )
        if close_col not in data.columns:
            raise ValueError(f"{close_col} missing")

        # --- Ú©Ù¾ÛŒ Ø§Ù…Ù† Ùˆ Ø³Ø±ÛŒ Ù‚ÛŒÙ…Øª/Ø²Ù…Ø§Ù† ---
        df = data.copy().reset_index(drop=True)
        close = df[close_col].astype(float).reset_index(drop=True)
        t_idx = pd.to_datetime(df[tcol]).reset_index(drop=True)

        # ----------------- Ø¨Ø±Ú†Ø³Ø¨â€ŒØ¯Ù‡ÛŒ -----------------
        if mode == "train":
            # y(t) = 1{ close(t+1) > close(t) }  Ø¨Ø§ NaN Ø¨Ø±Ø§ÛŒ Ø¢Ø®Ø±ÛŒÙ† Ø±Ø¯ÛŒÙ
            close_next = close.shift(-1)
            y = (close_next > close).astype("float64")
            y[close_next.isna()] = np.nan  # Ø¢Ø®Ø±ÛŒÙ† Ø±Ø¯ÛŒÙØŒ ØªØ§Ø±Ú¯Øª Ù†Ø¯Ø§Ø±Ø¯

            valid = y.notna()
            df = df.loc[valid].reset_index(drop=True)
            t_idx = t_idx.loc[valid].reset_index(drop=True)
            close = close.loc[valid].reset_index(drop=True)
            y = y.loc[valid].reset_index(drop=True)

            # --- PATCH: Ø­Ø°Ù Ø±ÙˆØ²Ù‡Ø§ÛŒ Â«ØªØ¹Ø·ÛŒÙ„â€ŒÙ…Ø§Ù†Ù†Ø¯Â» Ø¨Ø§ ØªØ§Ø±Ú¯Øª Ø«Ø§Ø¨Øª Ùˆ Ù†ÙˆØ³Ø§Ù† Ø¨Ø³ÛŒØ§Ø± Ú©Ù… ---
            # ØªØ¹Ø±ÛŒÙ Â«ØªØ¹Ø·ÛŒÙ„â€ŒÙ…Ø§Ù†Ù†Ø¯Â»:
            #   1) Ø¯Ø± Ø¢Ù† Ø±ÙˆØ² y ÙÙ‚Ø· ÛŒÚ© Ù…Ù‚Ø¯Ø§Ø± Ø¯Ø§Ø±Ø¯ (Ù‡Ù…Ù‡ 0 ÛŒØ§ Ù‡Ù…Ù‡ 1)
            #   2) Ø±ÙÙ†Ø¬ close Ø¢Ù† Ø±ÙˆØ² Ø¨Ø³ÛŒØ§Ø± Ú©ÙˆÚ†Ú© Ø§Ø³Øª (Ø¨Ø§Ø²Ø§Ø± ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ ÙÙ„Øª)
            day_idx = t_idx.dt.normalize()
            grp_df = pd.DataFrame({"y": y, "close": close, "day": day_idx})

            bad_days = []
            for d, g in grp_df.groupby("day"):
                if g["y"].nunique(dropna=True) <= 1:
                    c_range = float(g["close"].max() - g["close"].min())
                    c_level = float(max(1.0, g["close"].mean()))
                    # Ø±ÙÙ†Ø¬ Ú©Ù…ØªØ± Ø§Ø² 0.01% Ø³Ø·Ø­ Ù‚ÛŒÙ…Øª â‡’ Ø¨Ø§Ø²Ø§Ø± ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ Ø¨Ø³ØªÙ‡ Ø§Ø³Øª
                    if c_range <= 1e-4 * c_level:
                        bad_days.append(d)

            if bad_days:
                mask = ~day_idx.isin(bad_days)
                df = df.loc[mask].reset_index(drop=True)
                y = y.loc[mask].reset_index(drop=True)
                t_idx = t_idx.loc[mask].reset_index(drop=True)
                close = close.loc[mask].reset_index(drop=True)

        else:
            # Ø¯Ø± Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒØŒ y ÙˆØ§Ù‚Ø¹ÛŒ Ù„Ø§Ø²Ù… Ù†Ø¯Ø§Ø±ÛŒÙ…Ø› Ø¨Ø¹Ø¯Ø§Ù‹ ØµÙØ± Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            y = pd.Series(np.zeros(len(df), dtype=np.int8))

        # ----------------- Ø§Ù†ØªØ®Ø§Ø¨ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ ÙÛŒÚ†Ø± Ù¾Ø§ÛŒÙ‡ -----------------

        # ----------------- Ø§Ù†ØªØ®Ø§Ø¨ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ ÙÛŒÚ†Ø± Ù¾Ø§ÛŒÙ‡ -----------------
        time_tokens = ("hour", "day_of_week", "is_weekend")
        time_cols = [
            c
            for c in df.columns
            if c.endswith("_time")
            or c == "time"
            or any(tok in c for tok in time_tokens)
        ]
        base_candidates = [c for c in df.columns if c not in time_cols + [close_col]]

        # ÙÙ‚Ø· Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ (Ø­Ø°Ù datetime / objectØŒ Ù…Ø«Ù„ index Ø¨Ø§ dtype Ø¹Ø¬ÛŒØ¨)
        num_cols = (
            df[base_candidates]
            .select_dtypes(include=[np.number])
            .columns.tolist()
        )
        dropped_non_numeric = [c for c in base_candidates if c not in num_cols]
        if dropped_non_numeric:
            logging.getLogger(__name__).info(
                "[ready] dropping non-numeric base feature columns: %s",
                dropped_non_numeric,
            )
        base_candidates = num_cols

        # Ø¨Ù„Ø§Ú©â€ŒÙ„ÛŒØ³Øª ÙÛŒÚ†Ø±Ù‡Ø§ (Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§)
        black = _load_feature_blacklist()
        if black:
            base_candidates = [c for c in base_candidates if c not in black]


        # ----------------- ØªÙØ³ÛŒØ± selected_features -----------------
        import re as _re

        tminus_regex = _re.compile(r"_tminus\d+$")
        has_tminus = bool(
            selected_features
            and any(tminus_regex.search(str(f)) for f in selected_features)
        )

        if has_tminus:
            # selected_features Ù„ÛŒØ³Øª ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ù¾Ù†Ø¬Ø±Ù‡â€ŒØ§ÛŒ Ø§Ø³Øª (Ø¨Ø§ _tminusN)
            # Ø§ÛŒÙ†â€ŒØ¬Ø§ Ù¾Ø§ÛŒÙ‡â€ŒÛŒ Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ø¯Ø± df Ù¾ÛŒØ¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            base_from_sel = {tminus_regex.sub("", str(f)) for f in selected_features}
            feats_base = [c for c in base_candidates if c in base_from_sel]
        else:
            if selected_features is None:
                # ÙÙ‚Ø· Ø¯Ø± TRAIN Ø§Ù†ØªØ®Ø§Ø¨ ÙÛŒÚ†Ø± Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯
                if mode == "train":
                    feats_base = self.select_features(df[base_candidates], y)
                else:
                    feats_base = base_candidates
            elif selected_features == []:
                # "[]": ÛŒØ¹Ù†ÛŒ Ù‡Ù…Ù‡â€ŒÛŒ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
                feats_base = base_candidates
            else:
                # Ù„ÛŒØ³Øª Ø§Ø³Ø§Ù…ÛŒ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡
                feats_base = [c for c in selected_features if c in base_candidates]

        if not feats_base:
            logger = logging.getLogger(__name__)
            # Ø³Ù†Ø§Ø±ÛŒÙˆÛŒ Ø§ØµÙ„ÛŒ Ù…Ø´Ú©Ù„â€ŒØ³Ø§Ø²: TRAIN + selected_features=None
            # ÛŒØ¹Ù†ÛŒ select_features Ù‡ÛŒÚ† Ø³ØªÙˆÙ†ÛŒ Ø¨Ø±Ù†Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡Ø›
            # Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†â€ŒÚ©Ù‡ ÙØ§Ø² Ù†Ù‡Ø§ÛŒÛŒ GA Ù†Ø®ÙˆØ§Ø¨Ø¯ØŒ Ø±ÙˆÛŒ ØªÙ…Ø§Ù… base_candidates Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯ÛŒÙ….
            if (selected_features is None) and base_candidates:
                logger.warning(
                    "[ready] feats_base empty (mode=%s) â€“ falling back to all %d numeric base features",
                    mode,
                    len(base_candidates),
                )
                feats_base = base_candidates
            else:
                # Ø¯Ø± Ø³Ø§ÛŒØ± Ø­Ø§Ù„Øªâ€ŒÙ‡Ø§ Ø§Ú¯Ø± Ù‡Ù…Ú†Ù†Ø§Ù† ÙÛŒÚ†Ø±ÛŒ Ù†Ø¯Ø§Ø±ÛŒÙ…ØŒ Ù†Ø§Ú†Ø§Ø±ÛŒÙ… Ø®Ø±ÙˆØ¬ÛŒ Ø®Ø§Ù„ÛŒ Ø¨Ø¯Ù‡ÛŒÙ…
                if with_times:
                    return (
                        pd.DataFrame(),
                        pd.Series(dtype="int64"),
                        [],
                        pd.Series(dtype=float),
                        pd.Series(dtype="datetime64[ns]"),
                    )
                else:
                    return (
                        pd.DataFrame(),
                        pd.Series(dtype="int64"),
                        [],
                        pd.Series(dtype=float),
                    )
        X_base = df[feats_base].copy()

        # ----------------- Ù¾Ù†Ø¬Ø±Ù‡â€ŒØ¨Ù†Ø¯ÛŒ (window > 1) -----------------
        if window > 1:
            if len(X_base) < window:
                logging.warning(
                    "Not enough rows (%d) for window=%d", len(X_base), window
                )
                if with_times:
                    return (
                        pd.DataFrame(),
                        pd.Series(dtype="int64"),
                        [],
                        pd.Series(dtype=float),
                        pd.Series(dtype="datetime64[ns]"),
                    )
                else:
                    return (
                        pd.DataFrame(),
                        pd.Series(dtype="int64"),
                        [],
                        pd.Series(dtype=float),
                    )

            mats = [X_base.shift(i) for i in range(window)]
            Xw = (
                pd.concat(mats, axis=1)
                .iloc[window - 1 :]
                .reset_index(drop=True)
            )
            col_names = [
                f"{c}_tminus{i}" for i in range(window) for c in feats_base
            ]
            Xw.columns = col_names[: Xw.shape[1]]

            # Ù‡Ù…â€ŒØªØ±Ø§Ø² Ú©Ø±Ø¯Ù† y, Ø²Ù…Ø§Ù† Ùˆ Ù‚ÛŒÙ…Øª Ø¨Ø§ Ù¾Ù†Ø¬Ø±Ù‡
            y = y.iloc[window - 1 :].reset_index(drop=True)
            t_idx = t_idx.iloc[window - 1 :].reset_index(drop=True)
            close = close.iloc[window - 1 :].reset_index(drop=True)

            if selected_features and len(selected_features) > 0:
                # ÙÙ‚Ø· ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ø®ÙˆØ§Ø³ØªÙ‡ Ø´Ø¯Ù‡ Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø± (ØªØ±ØªÛŒØ¨ Ù‡Ù…Ø§Ù† selected_features)
                cols_keep = [c for c in selected_features if c in Xw.columns]
                X_f = Xw[cols_keep]
            else:
                X_f = Xw
        else:
            X_f = X_base

        # ----------------- Ù‡Ù…â€ŒÙ‚Ø¯ Ú©Ø±Ø¯Ù† Ù†Ù‡Ø§ÛŒÛŒ -----------------
        L = min(len(X_f), len(y), len(t_idx), len(close))
        X_f = X_f.iloc[:L].reset_index(drop=True)
        y = y.iloc[:L].reset_index(drop=True)
        t_idx = t_idx.iloc[:L].reset_index(drop=True)
        close = close.iloc[:L].reset_index(drop=True)

        # ----------------- drop-last Ø§Ø®ØªÛŒØ§Ø±ÛŒ -----------------
        if mode == "train" and train_drop_last and len(X_f) > 0:
            X_f = X_f.iloc[:-1].reset_index(drop=True)
            y = y.iloc[:-1].reset_index(drop=True)
            t_idx = t_idx.iloc[:-1].reset_index(drop=True)
            close = close.iloc[:-1].reset_index(drop=True)

        # ----------------- StrongFeatureSelector (ÙÙ‚Ø· TRAIN Ù†Ù‡Ø§ÛŒÛŒ) -----------------
        if (
            mode == "train"
            and apply_strong_fs
            and selected_features is None    # ÛŒØ¹Ù†ÛŒ Ù†Ù‡ CV Ø¯Ø± GAØŒ Ù†Ù‡ Ú©Ø§Ù„ Ø¨Ø§ Ù„ÛŒØ³Øª Ø®Ø§Øµ
            and X_f.shape[1] > int(strong_fs_max_features)
        ):
            fs_logger = logging.getLogger(__name__)
            MAX_FEATS = int(strong_fs_max_features)

            fs_logger.info(
                "[ready] StrongFeatureSelector input shape: rows=%d, cols=%d",
                X_f.shape[0],
                X_f.shape[1],
            )

            selector = StrongFeatureSelector(
                max_features=MAX_FEATS,
                pre_selection_factor=3,
                random_state=SEED,
                n_estimators=256,
                n_jobs=1,          # Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² oversubscription Ø¯Ø± GA
                corr_n_jobs=1,
            )

            try:
                X_selected = selector.fit_transform(X_f, y)
                selected_cols = list(X_selected.columns)

                if len(selected_cols) == 0:
                    fs_logger.warning(
                        "[ready] StrongFeatureSelector returned 0 columns â€“ "
                        "falling back to first %d features (out of %d).",
                        MAX_FEATS,
                        X_f.shape[1],
                    )
                    # fallback: truncate Ø¨Ù‡  MAX_FEATS
                    X_f = X_f.iloc[:, :MAX_FEATS].copy()
                else:
                    fs_logger.info(
                        "[ready] StrongFeatureSelector reduced features from %d to %d",
                        X_f.shape[1],
                        len(selected_cols),
                    )
                    X_f = X_selected

            except Exception as e:
                fs_logger.warning(
                    "[ready] StrongFeatureSelector failed (%s); "
                    "falling back to first %d features (out of %d).",
                    e,
                    MAX_FEATS,
                    X_f.shape[1],
                )
                if X_f.shape[1] > MAX_FEATS:
                    X_f = X_f.iloc[:, :MAX_FEATS].copy()

        if mode != "train":
            if predict_drop_last and len(X_f) > 0:
                X_f = X_f.iloc[:-1].reset_index(drop=True)
                t_idx = t_idx.iloc[:-1].reset_index(drop=True)
                close = close.iloc[:-1].reset_index(drop=True)
            # Ø¯Ø± Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒØŒ y Ù…ØµØ±Ù Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯ â†’ ØµÙØ± Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø±ÛŒÙ…
            y = np.zeros(len(X_f), dtype=np.int64)

        # ----------------- Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ -----------------
        feats_final = list(X_f.columns)

        if mode == "train":
            y = y.astype("int64")
            # Ø°Ø®ÛŒØ±Ù‡â€ŒÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø¹Ø¯ Ø§Ø² Ù¾Ù†Ø¬Ø±Ù‡ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ live / threshold / test
            self.train_columns_after_window = feats_final

        if with_times:
            return X_f, y, feats_final, close, t_idx
        else:
            return X_f, y, feats_final, close

    # ================= 4) READY_INCREMENTAL =================
    def ready_incremental(
        self,
        data_window: pd.DataFrame,
        window: int = 1,
        selected_features: List[str] | None = None,
        with_times: bool = False,
        predict_drop_last: bool = False,   # â— Ø¯ÛŒÙØ§Ù„Øª Ø¬Ø¯ÛŒØ¯ = False
    ):
        if not hasattr(self, "_live_prev2") or self._live_prev2 is None:
            self._live_prev2 = data_window.iloc[-2:].copy()
            return (pd.DataFrame(), [], None) if with_times else (pd.DataFrame(), [])

        concat = pd.concat([self._live_prev2, data_window], ignore_index=True)

        X_full, _, feats, price_raw, t_idx = self.ready(
            concat,
            window=window,
            selected_features=selected_features,
            mode="predict",
            with_times=True,
            predict_drop_last=predict_drop_last,
        )

        self._live_prev2 = data_window.iloc[-2:].copy()

        if X_full.empty:
            return (pd.DataFrame(), feats, None) if with_times else (pd.DataFrame(), feats)

        X_last = X_full.tail(1).reset_index(drop=True)
        t_feat = (
            pd.to_datetime(t_idx.iloc[-1])
            if (t_idx is not None and len(t_idx) > 0)
            else None
        )
        return (X_last, feats, t_feat) if with_times else (X_last, feats)

    # ================= 5) LOAD & MERGE =================
    def load_data(self) -> pd.DataFrame:
        if not self.filepaths or not isinstance(self.filepaths, dict):
            raise ValueError("[load_data] filepaths not provided or invalid")

        logging.info("[load_data] parallel load %d timeframes", len(self.filepaths))

        # â¬…ï¸ ÙÙ‚Ø· ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø±ÛŒÙ…Ø› Ù†Ø¨ÙˆØ¯Ù† 30T = Ø®Ø·Ø§ÛŒ ÙÙˆØ±ÛŒ
        existing = {tf: fp for tf, fp in self.filepaths.items() if os.path.exists(fp)}
        missing  = {tf: fp for tf, fp in self.filepaths.items() if tf not in existing}

        for tf, fp in missing.items():
            print(f"âš ï¸ File not found: {os.path.abspath(fp)}")

        if self.main_timeframe not in existing:
            raise FileNotFoundError(f"[load_data] Main timeframe '{self.main_timeframe}' file is missing: "
                                    f"{os.path.abspath(self.filepaths.get(self.main_timeframe, ''))}")

        # Ø§Ú¯Ø± ÙÙ‚Ø· 30T Ø¯Ø§Ø±ÛŒØŒ Ù‡Ù…ÛŒÙ† Ú©Ø§ÙÛŒâ€ŒØ³ØªØ› Ø¨Ù‚ÛŒÙ‡ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§ Ø§Ø®ØªÛŒØ§Ø±ÛŒ Ù‡Ø³ØªÙ†Ø¯
        self.filepaths = existing
        logging.info("[load_data] using %d existing timeframes (%s)",
                    len(self.filepaths), ", ".join(sorted(self.filepaths.keys())))

        # ---------- 1) Ù…ÙˆØ§Ø²ÛŒ-Ø®ÙˆØ§Ù†ÛŒ Ùˆ Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ù‡Ø± ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… ----------
        dfs = Parallel(
            n_jobs=min(mp.cpu_count(), len(self.filepaths)),
            backend="loky"
        )(
            delayed(self.load_and_process_timeframe)(tf, fp)
            for tf, fp in self.filepaths.items()
        )

        # ---------- 2) Ø§Ø¯ØºØ§Ù… Ø±ÙˆÛŒ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… Ø§ØµÙ„ÛŒ ----------
        main_tf  = self.main_timeframe
        tfs = list(self.filepaths.keys())

        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø§ÛŒÙ†Ø¯Ú©Ø³ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… Ø§ØµÙ„ÛŒ
        try:
            idx_main = tfs.index(self.main_timeframe)
        except ValueError:
            raise KeyError(f"Main timeframe '{self.main_timeframe}' not in filepaths")

        df0 = dfs[idx_main]
        main_tf = self.main_timeframe

        if f"{main_tf}_time" not in df0.columns:
            if "time" in df0.columns:
                df0[f"{main_tf}_time"] = pd.to_datetime(df0["time"], errors="coerce")
            else:
                raise KeyError(f"Missing '{main_tf}_time' and 'time' columns in main timeframe dataframe.")
        main_df = df0.set_index(f"{main_tf}_time", drop=False)

        # join Ø¨Ù‚ÛŒÙ‡â€ŒÛŒ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§
        for j, tf in enumerate(tfs):
            if j == idx_main:
                continue
            dfj = dfs[j]
            main_df = main_df.join(
                dfj.set_index(f"{tf}_time", drop=False),
                how="outer",
                rsuffix=f"_{tf}",
            )

        # ---------- 3) Ø­Ø°Ù Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Â«Ù†Ø§Ù¾Ø§ÛŒØ¯Ø§Ø±Â» Ú©Ù‡ Ù‚Ø¨Ù„Ø§Ù‹ Ø¨Ø±Ø§ÛŒ Ù‡Ø± TF Ú©Ø´Ù Ø´Ø¯Ù‡ ----------
        for tf, bad_set in self.bad_cols_tf.items():
            if not bad_set:
                continue
            cols_to_drop = [c for c in main_df.columns if c in bad_set]
            if cols_to_drop:
                main_df.drop(columns=cols_to_drop, inplace=True, errors="ignore")
                if self.verbose:
                    print(f"[DROP] Removed {len(cols_to_drop)} unstable cols from {tf}")

        # ---------- 4) Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ NaN/Inf Ùˆ ÙÙˆØ±ÙˆØ§Ø±Ø¯-Ù¾Ø±Ú©Ø±Ø¯Ù† ----------
        main_df.replace([np.inf, -np.inf], np.nan, inplace=True)
        main_df.ffill(inplace=True)
        main_df.dropna(how="all", inplace=True)

        # ---------- 5) Ø­Ø°Ù Ø³Ø·Ø±Ù‡Ø§ÛŒ ØªØ¹Ø·ÛŒÙ„Ø§Øª (Ø´Ù†Ø¨Ù‡/ÛŒÚ©Ø´Ù†Ø¨Ù‡) ----------
        if isinstance(main_df.index, pd.DatetimeIndex):
            main_df = main_df[~main_df.index.dayofweek.isin([5, 6])]
            main_df.ffill(inplace=True)

        # ---------- 6) Ù†Ù‡Ø§ÛŒÛŒâ€ŒØ³Ø§Ø²ÛŒ Ø§Ù†Ø¯ÛŒØ³ / Ø­Ø°Ù duplications ----------
        main_df.reset_index(drop=False, inplace=True)
        tcol = f"{main_tf}_time"
        if tcol not in main_df.columns:             # Ø§ÛŒÙ…Ù†ÛŒ Ø§Ú¯Ø± Ø³ØªÙˆÙ† Ø¬Ø§Ø¨Ù‡â€ŒØ¬Ø§ Ø´Ø¯
            main_df.rename(columns={main_df.columns[0]: tcol}, inplace=True)
        main_df = main_df.loc[~main_df[tcol].duplicated(keep="last")]

        logging.info("[load_data] Final shape=%s", main_df.shape)
        gc.collect()
        return main_df

    # ================= 6) OUTER INTERFACE =================
    def get_prepared_data(self, window=1, mode="train") -> Tuple[pd.DataFrame, pd.Series, List[str]]:
        """
        Ù„Ø§ÛŒÙ‡â€ŒÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ML / DL.

        - ØªÙ…Ø§Ù… Ù…Ù†Ø·Ù‚ Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ Ùˆ Ù¾Ù†Ø¬Ø±Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø± ready Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯.
        - Ø¯Ø± Ø­Ø§Ù„Øª TRAINØŒ Ø§Ú¯Ø± apply_strong_fs=True Ø¨Ø§Ø´Ø¯ØŒ ÛŒÚ© Ù…Ø±Ø­Ù„Ù‡â€ŒÛŒ
          StrongFeatureSelector Ø±ÙˆÛŒ Ù…Ø§ØªØ±ÛŒØ³ Ù†Ù‡Ø§ÛŒÛŒ X (Ø¨Ø¹Ø¯ Ø§Ø² window) Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯
          Ùˆ Ø­Ø¯Ø§Ú©Ø«Ø± strong_fs_max_features Ø³ØªÙˆÙ† Ù†Ú¯Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
        """
        merged = self.load_data()
        X, y, feats, _ = self.ready(
            merged,
            window=window,
            mode=mode,
            apply_strong_fs=(mode == "train"),
            strong_fs_max_features=300,
        )

        if mode == "train":
            # ready Ø®ÙˆØ¯Ø´ train_columns_after_window Ø±Ø§ Ø³Øª Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ
            # ÙˆÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† sync Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            self.train_columns_after_window = list(feats)

        return X, y, feats



if __name__ == "__main__":
    prep = PREPARE_DATA_FOR_TRAIN(verbose=True)
    X, y, f = prep.get_prepared_data(window=1, mode="train")
    print("Shapes:", X.shape, y.shape, len(f))

